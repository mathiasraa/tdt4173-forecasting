{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import (\n",
    "    load_data,\n",
    "    remove_duplicates_in_coloumn,\n",
    "    convert_from_degree_to_ciruclar,\n",
    "    resample_hourly,\n",
    "    create_time_features,\n",
    "    find_repeated_indices,\n",
    "    load_val_dates,\n",
    "    create_lag_features\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows\n",
    "for location in data.keys():\n",
    "    df = data[location]\n",
    "\n",
    "    df[\"y\"] = remove_duplicates_in_coloumn(df[\"y\"], \"time\")\n",
    "    df[\"X_test_estimated\"] = remove_duplicates_in_coloumn(df[\"X_test_estimated\"], \"date_forecast\")\n",
    "    df[\"X_train_estimated\"] = remove_duplicates_in_coloumn(df[\"X_train_estimated\"], \"date_forecast\")\n",
    "    df[\"X_train_observed\"] = remove_duplicates_in_coloumn(df[\"X_train_observed\"], \"date_forecast\")\n",
    "\n",
    "    data[location] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "columns_to_drop = [\n",
    "    \"ceiling_height_agl:m\",\n",
    "    \"cloud_base_agl:m\",\n",
    "    \"snow_density:kgm3\",\n",
    "    \"elevation:m\",\n",
    "    \"precip_5min:mm\",\n",
    "    \"precip_type_5min:idx\",\n",
    "    \"pressure_50m:hPa\",\n",
    "    \"snow_drift:idx\",\n",
    "    \"wind_speed_u_10m:ms\",\n",
    "    \"wind_speed_v_10m:ms\",\n",
    "    \"wind_speed_w_1000hPa:ms\",\n",
    "    \"date_calc\",\n",
    "\n",
    "    # Duplicate columns\n",
    "    \"diffuse_rad_1h:J\",\n",
    "    \"direct_rad_1h:J\",\n",
    "    \"clear_sky_energy_1h:J\",\n",
    "]\n",
    "\n",
    "for location in data.keys():\n",
    "    df = data[location]\n",
    "\n",
    "    df[\"X_test_estimated\"] = df[\"X_test_estimated\"].drop(columns=columns_to_drop, errors=\"ignore\")\n",
    "    df[\"X_train_estimated\"] = df[\"X_train_estimated\"].drop(columns=columns_to_drop, errors=\"ignore\")\n",
    "    df[\"X_train_observed\"] = df[\"X_train_observed\"].drop(columns=columns_to_drop, errors=\"ignore\")\n",
    "\n",
    "    data[location] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sun azimuth feature engineering\n",
    "for location in data.keys():\n",
    "    df = data[location]\n",
    "\n",
    "    df[\"X_test_estimated\"] = convert_from_degree_to_ciruclar(df[\"X_test_estimated\"], \"sun_azimuth:d\")\n",
    "    df[\"X_train_estimated\"] = convert_from_degree_to_ciruclar(df[\"X_train_estimated\"], \"sun_azimuth:d\")\n",
    "    df[\"X_train_observed\"] = convert_from_degree_to_ciruclar(df[\"X_train_observed\"], \"sun_azimuth:d\")\n",
    "\n",
    "    data[location] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce granularity of data to hourly\n",
    "for location in data.keys():\n",
    "    df = data[location]\n",
    "\n",
    "    df[\"X_test_estimated\"] = resample_hourly(df[\"X_test_estimated\"], func=\"sum\")\n",
    "    df[\"X_train_estimated\"] = resample_hourly(df[\"X_train_estimated\"], func=\"sum\")\n",
    "    df[\"X_train_observed\"] = resample_hourly(df[\"X_train_observed\"], func=\"sum\")\n",
    "\n",
    "    data[location] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date_forecast', 'absolute_humidity_2m:gm3', 'air_density_2m:kgm3',\n",
       "       'clear_sky_rad:W', 'dew_or_rime:idx', 'dew_point_2m:K', 'diffuse_rad:W',\n",
       "       'direct_rad:W', 'effective_cloud_cover:p', 'fresh_snow_12h:cm',\n",
       "       'fresh_snow_1h:cm', 'fresh_snow_24h:cm', 'fresh_snow_3h:cm',\n",
       "       'fresh_snow_6h:cm', 'is_day:idx', 'is_in_shadow:idx',\n",
       "       'msl_pressure:hPa', 'pressure_100m:hPa', 'prob_rime:p',\n",
       "       'rain_water:kgm2', 'relative_humidity_1000hPa:p', 'sfc_pressure:hPa',\n",
       "       'snow_depth:cm', 'snow_melt_10min:mm', 'snow_water:kgm2',\n",
       "       'sun_elevation:d', 'super_cooled_liquid_water:kgm2', 't_1000hPa:K',\n",
       "       'total_cloud_cover:p', 'visibility:m', 'wind_speed_10m:ms',\n",
       "       'sun_azimuth:d_sin', 'sun_azimuth:d_cos'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"A\"][\"X_test_estimated\"].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lag features\n",
    "for location in data.keys():\n",
    "    df = data[location]\n",
    "\n",
    "    df[\"X_test_estimated\"] = create_lag_features(\n",
    "        df[\"X_test_estimated\"],\n",
    "        columns=[\"effective_cloud_cover:p\", \"absolute_humidity_2m:gm3\"],\n",
    "        lag=-1\n",
    "    )\n",
    "    df[\"X_train_estimated\"] = create_lag_features(\n",
    "        df[\"X_train_estimated\"],\n",
    "        columns=[\"effective_cloud_cover:p\", \"absolute_humidity_2m:gm3\"],\n",
    "        lag=-1\n",
    "    )\n",
    "    df[\"X_train_observed\"] = create_lag_features(\n",
    "        df[\"X_train_observed\"],\n",
    "        columns=[\"effective_cloud_cover:p\", \"absolute_humidity_2m:gm3\"],\n",
    "        lag=-1\n",
    "    )\n",
    "\n",
    "    df[\"X_test_estimated\"] = create_lag_features(\n",
    "        df[\"X_test_estimated\"],\n",
    "        columns=[\"effective_cloud_cover:p\", \"absolute_humidity_2m:gm3\"],\n",
    "        lag=-2\n",
    "    )\n",
    "    df[\"X_train_estimated\"] = create_lag_features(\n",
    "        df[\"X_train_estimated\"],\n",
    "        columns=[\"effective_cloud_cover:p\", \"absolute_humidity_2m:gm3\"],\n",
    "        lag=-2\n",
    "    )\n",
    "    df[\"X_train_observed\"] = create_lag_features(\n",
    "        df[\"X_train_observed\"],\n",
    "        columns=[\"effective_cloud_cover:p\", \"absolute_humidity_2m:gm3\"],\n",
    "        lag=-2\n",
    "    )\n",
    "\n",
    "    data[location] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns train_targets, observed and estimated sets left after filtering away NaN\n",
    "def drop_nan_rows_in_target_and_train(df):\n",
    "    df[\"y\"] = df[\"y\"].dropna(subset=[\"pv_measurement\"])\n",
    "    valid_dates = df[\"y\"][\"time\"]\n",
    "\n",
    "    df[\"X_train_observed\"] = df[\"X_train_observed\"][\n",
    "        df[\"X_train_observed\"][\"date_forecast\"].isin(valid_dates)\n",
    "    ]\n",
    "    df[\"X_train_estimated\"] = df[\"X_train_estimated\"][\n",
    "        df[\"X_train_estimated\"][\"date_forecast\"].isin(valid_dates)\n",
    "    ]\n",
    "    df[\"y\"] = df[\"y\"][\n",
    "        df[\"y\"][\"time\"].isin(df[\"X_train_observed\"][\"date_forecast\"])\n",
    "        | df[\"y\"][\"time\"].isin(df[\"X_train_estimated\"][\"date_forecast\"])\n",
    "    ]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "for location in data.keys():\n",
    "    df = data[location]\n",
    "\n",
    "    df = drop_nan_rows_in_target_and_train(df)\n",
    "\n",
    "    data[location] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time features\n",
    "for location in data.keys():\n",
    "    df = data[location]\n",
    "\n",
    "    df[\"X_test_estimated\"] = create_time_features(df[\"X_test_estimated\"], \"date_forecast\")\n",
    "    df[\"X_train_estimated\"] = create_time_features(df[\"X_train_estimated\"], \"date_forecast\")\n",
    "    df[\"X_train_observed\"] = create_time_features(df[\"X_train_observed\"], \"date_forecast\")\n",
    "\n",
    "    data[location] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      "(4418, 44) (29667, 44) (34085, 2)\n",
      "B:\n",
      "(3625, 44) (29218, 44) (32843, 2)\n",
      "C:\n",
      "(2954, 44) (23141, 44) (26095, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"A:\")\n",
    "print(data[\"A\"][\"X_train_estimated\"].shape, data[\"A\"][\"X_train_observed\"].shape, data[\"A\"][\"y\"].shape)\n",
    "\n",
    "print(\"B:\")\n",
    "print(data[\"B\"][\"X_train_estimated\"].shape, data[\"B\"][\"X_train_observed\"].shape, data[\"B\"][\"y\"].shape)\n",
    "\n",
    "print(\"C:\")\n",
    "print(data[\"C\"][\"X_train_estimated\"].shape, data[\"C\"][\"X_train_observed\"].shape, data[\"C\"][\"y\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making training and validation data for A\n",
    "\n",
    "X_train = pd.DataFrame()\n",
    "y_train = pd.DataFrame()\n",
    "\n",
    "X_validate = pd.DataFrame()\n",
    "y_validate = pd.DataFrame()\n",
    "\n",
    "for location in data.keys():\n",
    "    percent_observed_train_a = 1\n",
    "    percent_estimated_train_a = 1\n",
    "\n",
    "    split_index_obs = int(\n",
    "        len(data[location][\"X_train_observed\"]) * percent_observed_train_a\n",
    "    )\n",
    "    split_index_est = int(\n",
    "        len(data[location][\"X_train_estimated\"]) * percent_estimated_train_a\n",
    "    )\n",
    "\n",
    "    X_train_observed_first_75 = data[location][\"X_train_observed\"][:split_index_obs]\n",
    "    X_train_observed_last_25 = data[location][\"X_train_observed\"][split_index_obs:]\n",
    "\n",
    "    X_train_estimated_first_75 = data[location][\"X_train_estimated\"][:split_index_est]\n",
    "    X_train_estimated_last_25 = data[location][\"X_train_estimated\"][split_index_est:]\n",
    "\n",
    "    X_train_loc = pd.concat([X_train_observed_first_75, X_train_estimated_first_75])\n",
    "    y_train_loc = data[location][\"y\"][\n",
    "        data[location][\"y\"][\"time\"].isin(X_train_loc[\"date_forecast\"])\n",
    "    ]\n",
    "\n",
    "    X_validate_loc = pd.concat([X_train_observed_last_25, X_train_estimated_last_25])\n",
    "    y_validate_loc = data[location][\"y\"][\n",
    "        data[location][\"y\"][\"time\"].isin(X_validate_loc[\"date_forecast\"])\n",
    "    ]\n",
    "\n",
    "    repeated_indices = find_repeated_indices(y_train_loc, \"pv_measurement\", 24)\n",
    "    y_train_loc = y_train_loc.reset_index()\n",
    "    y_train_loc = y_train_loc.drop(repeated_indices)\n",
    "    X_train_loc = X_train_loc[X_train_loc[\"date_forecast\"].isin(y_train_loc[\"time\"])]\n",
    "\n",
    "    repeated_indices = find_repeated_indices(y_validate_loc, \"pv_measurement\", 24)\n",
    "    y_validate_loc = y_validate_loc.reset_index()\n",
    "    y_validate_loc = y_validate_loc.drop(repeated_indices)\n",
    "    X_validate_loc = X_validate_loc[\n",
    "        X_validate_loc[\"date_forecast\"].isin(y_validate_loc[\"time\"])\n",
    "    ]\n",
    "\n",
    "    y_train_loc.reset_index(drop=True, inplace=True)\n",
    "    X_train_loc.reset_index(drop=True, inplace=True)\n",
    "    y_validate_loc.reset_index(drop=True, inplace=True)\n",
    "    X_validate_loc.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    X_train_loc[\"location\"] = location\n",
    "    y_train_loc[\"location\"] = location\n",
    "    X_validate_loc[\"location\"] = location\n",
    "    y_validate_loc[\"location\"] = location\n",
    "\n",
    "    X_train_loc.drop(\"date_forecast\", axis=1, inplace=True)\n",
    "    y_train_loc.drop(\"time\", axis=1, inplace=True)\n",
    "    X_validate_loc.drop(\"date_forecast\", axis=1, inplace=True)\n",
    "    y_validate_loc.drop(\"time\", axis=1, inplace=True)\n",
    "\n",
    "    X_train_loc = X_train_loc.reset_index().drop(columns=\"index\")\n",
    "    one_hot = pd.get_dummies(X_train_loc[\"location\"]).astype(int)\n",
    "    X_train_loc = X_train_loc.drop(\"location\", axis=1)\n",
    "    X_train_loc = pd.merge(X_train_loc, one_hot, left_index=True, right_index=True)\n",
    "\n",
    "    X_train = pd.concat([X_train_loc, X_train])\n",
    "    y_train = pd.concat([y_train_loc, y_train])\n",
    "    X_validate = pd.concat([X_validate_loc, X_validate])\n",
    "    y_validate = pd.concat([y_validate_loc, y_validate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"A\"][\"X_test_estimated\"][\"location\"] = \"A\"\n",
    "data[\"B\"][\"X_test_estimated\"][\"location\"] = \"B\"\n",
    "data[\"C\"][\"X_test_estimated\"][\"location\"] = \"C\"\n",
    "\n",
    "X_test = pd.concat([data[\"A\"][\"X_test_estimated\"], data[\"B\"][\"X_test_estimated\"], data[\"C\"][\"X_test_estimated\"]])\n",
    "# filtering out invalid dates:\n",
    "X_test = X_test[X_test[\"date_forecast\"].isin(load_val_dates())]\n",
    "# removing forecast coloum\n",
    "X_test = X_test.drop(\"date_forecast\", axis=1)\n",
    "\n",
    "X_test = X_test.reset_index().drop(columns=\"index\")\n",
    "one_hot = pd.get_dummies(X_test[\"location\"]).astype(int)\n",
    "X_test = X_test.drop(\"location\", axis=1)\n",
    "X_test = pd.merge(X_test, one_hot, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_exclude = [\"A\", \"B\", \"C\", \"dew_or_rime:idx\", \"is_day:idx\", \"_in_shadow:idx\"]\n",
    "\n",
    "columns_to_normalize = [col for col in X_train.columns if col not in columns_to_exclude]\n",
    "\n",
    "#Min-max\n",
    "# Calculate min and max values for scaling\n",
    "X_min = X_train[columns_to_normalize].min()\n",
    "X_max = X_train[columns_to_normalize].max()\n",
    "\n",
    "# Apply min-max scaling to the columns to be normalized\n",
    "X_train[columns_to_normalize] = (X_train[columns_to_normalize] - X_min) / (X_max - X_min)\n",
    "X_validate[columns_to_normalize] = (X_validate[columns_to_normalize] - X_min) / (X_max - X_min)\n",
    "X_test[columns_to_normalize] = (X_test[columns_to_normalize] - X_min) / (X_max - X_min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y_train[\"pv_measurement\"] = y_scaler.fit_transform(y_train[\"pv_measurement\"].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index().drop(columns=\"index\")\n",
    "y_train = y_train.reset_index().drop(columns=\"index\")\n",
    "new_train = pd.merge(X_train, y_train[\"pv_measurement\"], left_index=True, right_index=True)\n",
    "new_train = new_train.fillna(0)\n",
    "\n",
    "X_validate = X_validate.reset_index().drop(columns=\"index\")\n",
    "y_validate = y_validate.reset_index().drop(columns=\"index\")\n",
    "new_validate = pd.merge(X_validate, y_validate[\"pv_measurement\"], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train.to_csv(\"../data/processed/train.csv\", index=False)\n",
    "X_test.to_csv(\"../data/processed/X_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeated_indices_c.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D AWESOME DEEEEP NEURAL NEWORK\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "\n",
    "  # Define the model architecture\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(256, activation='relu'))\n",
    "  model.add(Dense(128, activation='relu'))\n",
    "  model.add(Dense(no_classes, activation='softmax'))\n",
    "\n",
    "  # Compile the model\n",
    "  model.compile(loss=loss_function,\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "\n",
    "  # Generate a print\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "  # Fit data to model\n",
    "  history = model.fit(inputs[train], targets[train],\n",
    "              batch_size=batch_size,\n",
    "              epochs=no_epochs,\n",
    "              verbose=verbosity)\n",
    "\n",
    "  # Generate generalization metrics\n",
    "  scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "  acc_per_fold.append(scores[1] * 100)\n",
    "  loss_per_fold.append(scores[0])\n",
    "\n",
    "  # Increase fold number\n",
    "  fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"17.0.1\" 2021-10-19; OpenJDK Runtime Environment Temurin-17.0.1+12 (build 17.0.1+12); OpenJDK 64-Bit Server VM Temurin-17.0.1+12 (build 17.0.1+12, mixed mode)\n",
      "  Starting server from /Users/mathiasraa/anaconda3/envs/forecasting/lib/python3.10/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /var/folders/xd/z_ptq9v136q7kj9lf2f4sblh0000gn/T/tmpiauhqxnm\n",
      "  JVM stdout: /var/folders/xd/z_ptq9v136q7kj9lf2f4sblh0000gn/T/tmpiauhqxnm/h2o_mathiasraa_started_from_python.out\n",
      "  JVM stderr: /var/folders/xd/z_ptq9v136q7kj9lf2f4sblh0000gn/T/tmpiauhqxnm/h2o_mathiasraa_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "#h2o-table-5.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-5 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-5 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-5 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-5 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-5 .h2o-table th,\n",
       "#h2o-table-5 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-5 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-5\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption></caption>\n",
       "    <thead></thead>\n",
       "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>02 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Europe/Oslo</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.42.0.4</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>12 days</td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_mathiasraa_e9goa0</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>4 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.10.4 final</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n"
      ],
      "text/plain": [
       "--------------------------  ---------------------------------\n",
       "H2O_cluster_uptime:         02 secs\n",
       "H2O_cluster_timezone:       Europe/Oslo\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.42.0.4\n",
       "H2O_cluster_version_age:    12 days\n",
       "H2O_cluster_name:           H2O_from_python_mathiasraa_e9goa0\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    4 Gb\n",
       "H2O_cluster_total_cores:    8\n",
       "H2O_cluster_allowed_cores:  8\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://127.0.0.1:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "Python_version:             3.10.4 final\n",
       "--------------------------  ---------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "AutoML progress: |\n",
      "14:43:44.451: AutoML: XGBoost is not available; skipping it.\n",
      "\n",
      "███████████████████████████████████████████████████████████████| (done) 100%\n",
      "gbm prediction progress: |███████████████████████████████████████████████████████| (done) 100%\n"
     ]
    }
   ],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "h2o.init()\n",
    "\n",
    "h2o_train= h2o.H2OFrame(new_train)\n",
    "h2o_test = h2o.H2OFrame(X_test)\n",
    "\n",
    "aml = H2OAutoML(max_models=20, seed=1, stopping_metric=\"MAE\", sort_metric=\"MAE\", stopping_tolerance=0.01)\n",
    "# aml.train(x=h2o_train.columns, y=\"pv_measurement\", training_frame=h2o_train, validation_frame=h2o_validate)\n",
    "aml.train(x=h2o_train.columns, y=\"pv_measurement\", training_frame=h2o_train)\n",
    "lb = aml.leaderboard\n",
    "preds = aml.leader.predict(h2o_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "gbm prediction progress: |███████████████████████████████████████████████████████| (done) 100%\n"
     ]
    }
   ],
   "source": [
    "aml.leader.predict(h2o.H2OFrame(new_train)).as_data_frame().to_csv(\"../data/comparison/model_local.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class='dataframe'>\n",
       "<thead>\n",
       "<tr><th>model_id                                               </th><th style=\"text-align: right;\">      mae</th><th style=\"text-align: right;\">     rmse</th><th style=\"text-align: right;\">       mse</th><th style=\"text-align: right;\">    rmsle</th><th style=\"text-align: right;\">  mean_residual_deviance</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>GBM_grid_1_AutoML_1_20231016_144344_model_5            </td><td style=\"text-align: right;\">0.0140406</td><td style=\"text-align: right;\">0.0424401</td><td style=\"text-align: right;\">0.00180116</td><td style=\"text-align: right;\">0.0313323</td><td style=\"text-align: right;\">              0.00180116</td></tr>\n",
       "<tr><td>StackedEnsemble_BestOfFamily_1_AutoML_1_20231016_144344</td><td style=\"text-align: right;\">0.0143641</td><td style=\"text-align: right;\">0.0423158</td><td style=\"text-align: right;\">0.00179063</td><td style=\"text-align: right;\">0.0312811</td><td style=\"text-align: right;\">              0.00179063</td></tr>\n",
       "<tr><td>GBM_4_AutoML_1_20231016_144344                         </td><td style=\"text-align: right;\">0.014453 </td><td style=\"text-align: right;\">0.0433035</td><td style=\"text-align: right;\">0.00187519</td><td style=\"text-align: right;\">0.0320185</td><td style=\"text-align: right;\">              0.00187519</td></tr>\n",
       "<tr><td>StackedEnsemble_AllModels_1_AutoML_1_20231016_144344   </td><td style=\"text-align: right;\">0.0145703</td><td style=\"text-align: right;\">0.0411439</td><td style=\"text-align: right;\">0.00169282</td><td style=\"text-align: right;\">0.0304413</td><td style=\"text-align: right;\">              0.00169282</td></tr>\n",
       "<tr><td>GBM_3_AutoML_1_20231016_144344                         </td><td style=\"text-align: right;\">0.014981 </td><td style=\"text-align: right;\">0.0441991</td><td style=\"text-align: right;\">0.00195356</td><td style=\"text-align: right;\">0.0326893</td><td style=\"text-align: right;\">              0.00195356</td></tr>\n",
       "<tr><td>DRF_1_AutoML_1_20231016_144344                         </td><td style=\"text-align: right;\">0.0151076</td><td style=\"text-align: right;\">0.0446005</td><td style=\"text-align: right;\">0.0019892 </td><td style=\"text-align: right;\">0.0330795</td><td style=\"text-align: right;\">              0.0019892 </td></tr>\n",
       "<tr><td>GBM_1_AutoML_1_20231016_144344                         </td><td style=\"text-align: right;\">0.0152928</td><td style=\"text-align: right;\">0.0451105</td><td style=\"text-align: right;\">0.00203496</td><td style=\"text-align: right;\">0.0333356</td><td style=\"text-align: right;\">              0.00203496</td></tr>\n",
       "<tr><td>GBM_2_AutoML_1_20231016_144344                         </td><td style=\"text-align: right;\">0.0155128</td><td style=\"text-align: right;\">0.0454086</td><td style=\"text-align: right;\">0.00206194</td><td style=\"text-align: right;\">0.0335916</td><td style=\"text-align: right;\">              0.00206194</td></tr>\n",
       "<tr><td>GBM_grid_1_AutoML_1_20231016_144344_model_4            </td><td style=\"text-align: right;\">0.0155716</td><td style=\"text-align: right;\">0.042865 </td><td style=\"text-align: right;\">0.00183741</td><td style=\"text-align: right;\">0.031767 </td><td style=\"text-align: right;\">              0.00183741</td></tr>\n",
       "<tr><td>GBM_5_AutoML_1_20231016_144344                         </td><td style=\"text-align: right;\">0.0160482</td><td style=\"text-align: right;\">0.0463692</td><td style=\"text-align: right;\">0.0021501 </td><td style=\"text-align: right;\">0.0343221</td><td style=\"text-align: right;\">              0.0021501 </td></tr>\n",
       "</tbody>\n",
       "</table><pre style='font-size: smaller; margin-bottom: 1em;'>[22 rows x 6 columns]</pre>"
      ],
      "text/plain": [
       "model_id                                                       mae       rmse         mse      rmsle    mean_residual_deviance\n",
       "-------------------------------------------------------  ---------  ---------  ----------  ---------  ------------------------\n",
       "GBM_grid_1_AutoML_1_20231016_144344_model_5              0.0140406  0.0424401  0.00180116  0.0313323                0.00180116\n",
       "StackedEnsemble_BestOfFamily_1_AutoML_1_20231016_144344  0.0143641  0.0423158  0.00179063  0.0312811                0.00179063\n",
       "GBM_4_AutoML_1_20231016_144344                           0.014453   0.0433035  0.00187519  0.0320185                0.00187519\n",
       "StackedEnsemble_AllModels_1_AutoML_1_20231016_144344     0.0145703  0.0411439  0.00169282  0.0304413                0.00169282\n",
       "GBM_3_AutoML_1_20231016_144344                           0.014981   0.0441991  0.00195356  0.0326893                0.00195356\n",
       "DRF_1_AutoML_1_20231016_144344                           0.0151076  0.0446005  0.0019892   0.0330795                0.0019892\n",
       "GBM_1_AutoML_1_20231016_144344                           0.0152928  0.0451105  0.00203496  0.0333356                0.00203496\n",
       "GBM_2_AutoML_1_20231016_144344                           0.0155128  0.0454086  0.00206194  0.0335916                0.00206194\n",
       "GBM_grid_1_AutoML_1_20231016_144344_model_4              0.0155716  0.042865   0.00183741  0.031767                 0.00183741\n",
       "GBM_5_AutoML_1_20231016_144344                           0.0160482  0.0463692  0.0021501   0.0343221                0.0021501\n",
       "[22 rows x 6 columns]\n"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method H2OCluster.shutdown of <h2o.backend.cluster.H2OCluster object at 0x2944dd3c0>>"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2o.cluster().shutdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm prediction progress: |███████████████████████████████████████████████████████| (done) 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.013178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.063545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155</th>\n",
       "      <td>0.011058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2156</th>\n",
       "      <td>0.006231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>0.002533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>0.001854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2159</th>\n",
       "      <td>0.001625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2160 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       predict\n",
       "0    -0.000001\n",
       "1     0.000014\n",
       "2     0.000079\n",
       "3     0.013178\n",
       "4     0.063545\n",
       "...        ...\n",
       "2155  0.011058\n",
       "2156  0.006231\n",
       "2157  0.002533\n",
       "2158  0.001854\n",
       "2159  0.001625\n",
       "\n",
       "[2160 rows x 1 columns]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = aml.leader.predict(h2o_test).as_data_frame()\n",
    "preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_csv(\"../data/results/predictions_2.csv\").drop(columns=\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df_scaled = y_scaler.inverse_transform(predictions)\n",
    "prediction_df_scaled_df = pd.DataFrame(prediction_df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>49.371084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>385.476516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  prediction\n",
       "0   0    0.000000\n",
       "1   1    0.000000\n",
       "2   2    0.000000\n",
       "3   3   49.371084\n",
       "4   4  385.476516"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultframe = pd.DataFrame(columns = [\"id\", \"prediction\"])\n",
    "resultframe[\"prediction\"] = prediction_df_scaled_df\n",
    "resultframe['prediction'] = np.where(resultframe['prediction'] < 0, 0, resultframe['prediction'])\n",
    "resultframe[\"id\"] = range(len(resultframe))\n",
    "resultframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "resultframe.to_csv(\"../data/results/\"+ str(datetime.datetime.now()) + \"-submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
