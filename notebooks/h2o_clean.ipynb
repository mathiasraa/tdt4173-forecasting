{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import (\n",
    "    load_data,\n",
    "    remove_duplicates_in_coloumn,\n",
    "    convert_from_degree_to_ciruclar,\n",
    "    resample_hourly,\n",
    "    create_time_features,\n",
    "    find_repeated_indices,\n",
    "    load_val_dates\n",
    ")\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows\n",
    "for location in data.keys():\n",
    "    df = data[location]\n",
    "\n",
    "    df[\"y\"] = remove_duplicates_in_coloumn(df[\"y\"], \"time\")\n",
    "    df[\"X_test_estimated\"] = remove_duplicates_in_coloumn(df[\"X_test_estimated\"], \"date_forecast\")\n",
    "    df[\"X_train_estimated\"] = remove_duplicates_in_coloumn(df[\"X_train_estimated\"], \"date_forecast\")\n",
    "    df[\"X_train_observed\"] = remove_duplicates_in_coloumn(df[\"X_train_observed\"], \"date_forecast\")\n",
    "\n",
    "    data[location] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "columns_to_drop = [\n",
    "    \"ceiling_height_agl:m\",\n",
    "    \"cloud_base_agl:m\",\n",
    "    \"snow_density:kgm3\",\n",
    "    \"elevation:m\",\n",
    "    \"precip_5min:mm\",\n",
    "    \"precip_type_5min:idx\",\n",
    "    \"pressure_50m:hPa\",\n",
    "    \"snow_drift:idx\",\n",
    "    \"wind_speed_u_10m:ms\",\n",
    "    \"wind_speed_v_10m:ms\",\n",
    "    \"wind_speed_w_1000hPa:ms\",\n",
    "    \"date_calc\"\n",
    "]\n",
    "\n",
    "for location in data.keys():\n",
    "    df = data[location]\n",
    "\n",
    "    df[\"X_test_estimated\"] = df[\"X_test_estimated\"].drop(columns=columns_to_drop, errors=\"ignore\")\n",
    "    df[\"X_train_estimated\"] = df[\"X_train_estimated\"].drop(columns=columns_to_drop, errors=\"ignore\")\n",
    "    df[\"X_train_observed\"] = df[\"X_train_observed\"].drop(columns=columns_to_drop, errors=\"ignore\")\n",
    "\n",
    "    data[location] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sun azimuth feature engineering\n",
    "for location in data.keys():\n",
    "    df = data[location]\n",
    "\n",
    "    df[\"X_test_estimated\"] = convert_from_degree_to_ciruclar(df[\"X_test_estimated\"], \"sun_azimuth:d\")\n",
    "    df[\"X_train_estimated\"] = convert_from_degree_to_ciruclar(df[\"X_train_estimated\"], \"sun_azimuth:d\")\n",
    "    df[\"X_train_observed\"] = convert_from_degree_to_ciruclar(df[\"X_train_observed\"], \"sun_azimuth:d\")\n",
    "\n",
    "    data[location] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce granularity of data to hourly\n",
    "for location in data.keys():\n",
    "    df = data[location]\n",
    "\n",
    "    df[\"X_test_estimated\"] = resample_hourly(df[\"X_test_estimated\"], func=\"sum\")\n",
    "    df[\"X_train_estimated\"] = resample_hourly(df[\"X_train_estimated\"], func=\"sum\")\n",
    "    df[\"X_train_observed\"] = resample_hourly(df[\"X_train_observed\"], func=\"sum\")\n",
    "\n",
    "    data[location] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns train_targets, observed and estimated sets left after filtering away NaN\n",
    "def drop_nan_rows_in_target_and_train(df):\n",
    "    df[\"y\"] = df[\"y\"].dropna(subset=[\"pv_measurement\"])\n",
    "    valid_dates = df[\"y\"][\"time\"]\n",
    "\n",
    "    df[\"X_train_observed\"] = df[\"X_train_observed\"][\n",
    "        df[\"X_train_observed\"][\"date_forecast\"].isin(valid_dates)\n",
    "    ]\n",
    "    df[\"X_train_estimated\"] = df[\"X_train_estimated\"][\n",
    "        df[\"X_train_estimated\"][\"date_forecast\"].isin(valid_dates)\n",
    "    ]\n",
    "    df[\"y\"] = df[\"y\"][\n",
    "        df[\"y\"][\"time\"].isin(df[\"X_train_observed\"][\"date_forecast\"])\n",
    "        | df[\"y\"][\"time\"].isin(df[\"X_train_estimated\"][\"date_forecast\"])\n",
    "    ]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "for location in data.keys():\n",
    "    df = data[location]\n",
    "\n",
    "    df = drop_nan_rows_in_target_and_train(df)\n",
    "\n",
    "    data[location] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time features\n",
    "for location in data.keys():\n",
    "    df = data[location]\n",
    "\n",
    "    df[\"X_test_estimated\"] = create_time_features(df[\"X_test_estimated\"], \"date_forecast\")\n",
    "    df[\"X_train_estimated\"] = create_time_features(df[\"X_train_estimated\"], \"date_forecast\")\n",
    "    df[\"X_train_observed\"] = create_time_features(df[\"X_train_observed\"], \"date_forecast\")\n",
    "\n",
    "    data[location] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      "(4418, 43) (29667, 43) (34085, 2)\n",
      "B:\n",
      "(3625, 43) (29218, 43) (32843, 2)\n",
      "C:\n",
      "(2954, 43) (23141, 43) (26095, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"A:\")\n",
    "print(data[\"A\"][\"X_train_estimated\"].shape, data[\"A\"][\"X_train_observed\"].shape, data[\"A\"][\"y\"].shape)\n",
    "\n",
    "print(\"B:\")\n",
    "print(data[\"B\"][\"X_train_estimated\"].shape, data[\"B\"][\"X_train_observed\"].shape, data[\"B\"][\"y\"].shape)\n",
    "\n",
    "print(\"C:\")\n",
    "print(data[\"C\"][\"X_train_estimated\"].shape, data[\"C\"][\"X_train_observed\"].shape, data[\"C\"][\"y\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making training and validation data for A\n",
    "\n",
    "X_train = pd.DataFrame()\n",
    "y_train = pd.DataFrame()\n",
    "\n",
    "X_validate = pd.DataFrame()\n",
    "y_validate = pd.DataFrame()\n",
    "\n",
    "for location in data.keys():\n",
    "    percent_observed_train_a = 1\n",
    "    percent_estimated_train_a = 1\n",
    "\n",
    "    split_index_obs = int(\n",
    "        len(data[location][\"X_train_observed\"]) * percent_observed_train_a\n",
    "    )\n",
    "    split_index_est = int(\n",
    "        len(data[location][\"X_train_estimated\"]) * percent_estimated_train_a\n",
    "    )\n",
    "\n",
    "    X_train_observed_first_75 = data[location][\"X_train_observed\"][:split_index_obs]\n",
    "    X_train_observed_last_25 = data[location][\"X_train_observed\"][split_index_obs:]\n",
    "\n",
    "    X_train_estimated_first_75 = data[location][\"X_train_estimated\"][:split_index_est]\n",
    "    X_train_estimated_last_25 = data[location][\"X_train_estimated\"][split_index_est:]\n",
    "\n",
    "    X_train_loc = pd.concat([X_train_observed_first_75, X_train_estimated_first_75])\n",
    "    y_train_loc = data[location][\"y\"][\n",
    "        data[location][\"y\"][\"time\"].isin(X_train_loc[\"date_forecast\"])\n",
    "    ]\n",
    "\n",
    "    X_validate_loc = pd.concat([X_train_observed_last_25, X_train_estimated_last_25])\n",
    "    y_validate_loc = data[location][\"y\"][\n",
    "        data[location][\"y\"][\"time\"].isin(X_validate_loc[\"date_forecast\"])\n",
    "    ]\n",
    "\n",
    "    repeated_indices = find_repeated_indices(y_train_loc, \"pv_measurement\", 24)\n",
    "    y_train_loc = y_train_loc.reset_index()\n",
    "    y_train_loc = y_train_loc.drop(repeated_indices)\n",
    "    X_train_loc = X_train_loc[X_train_loc[\"date_forecast\"].isin(y_train_loc[\"time\"])]\n",
    "\n",
    "    repeated_indices = find_repeated_indices(y_validate_loc, \"pv_measurement\", 24)\n",
    "    y_validate_loc = y_validate_loc.reset_index()\n",
    "    y_validate_loc = y_validate_loc.drop(repeated_indices)\n",
    "    X_validate_loc = X_validate_loc[\n",
    "        X_validate_loc[\"date_forecast\"].isin(y_validate_loc[\"time\"])\n",
    "    ]\n",
    "\n",
    "    y_train_loc.reset_index(drop=True, inplace=True)\n",
    "    X_train_loc.reset_index(drop=True, inplace=True)\n",
    "    y_validate_loc.reset_index(drop=True, inplace=True)\n",
    "    X_validate_loc.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    X_train_loc[\"location\"] = location\n",
    "    y_train_loc[\"location\"] = location\n",
    "    X_validate_loc[\"location\"] = location\n",
    "    y_validate_loc[\"location\"] = location\n",
    "\n",
    "    X_train_loc.drop(\"date_forecast\", axis=1, inplace=True)\n",
    "    y_train_loc.drop(\"time\", axis=1, inplace=True)\n",
    "    X_validate_loc.drop(\"date_forecast\", axis=1, inplace=True)\n",
    "    y_validate_loc.drop(\"time\", axis=1, inplace=True)\n",
    "\n",
    "    X_train_loc = X_train_loc.reset_index().drop(columns=\"index\")\n",
    "    one_hot = pd.get_dummies(X_train_loc[\"location\"]).astype(int)\n",
    "    X_train_loc = X_train_loc.drop(\"location\", axis=1)\n",
    "    X_train_loc = pd.merge(X_train_loc, one_hot, left_index=True, right_index=True)\n",
    "\n",
    "    X_train = pd.concat([X_train_loc, X_train])\n",
    "    y_train = pd.concat([y_train_loc, y_train])\n",
    "    X_validate = pd.concat([X_validate_loc, X_validate])\n",
    "    y_validate = pd.concat([y_validate_loc, y_validate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"A\"][\"X_test_estimated\"][\"location\"] = \"A\"\n",
    "data[\"B\"][\"X_test_estimated\"][\"location\"] = \"B\"\n",
    "data[\"C\"][\"X_test_estimated\"][\"location\"] = \"C\"\n",
    "\n",
    "X_test = pd.concat([data[\"A\"][\"X_test_estimated\"], data[\"B\"][\"X_test_estimated\"], data[\"C\"][\"X_test_estimated\"]])\n",
    "# filtering out invalid dates:\n",
    "X_test = X_test[X_test[\"date_forecast\"].isin(load_val_dates())]\n",
    "# removing forecast coloum\n",
    "X_test = X_test.drop(\"date_forecast\", axis=1)\n",
    "\n",
    "X_test = X_test.reset_index().drop(columns=\"index\")\n",
    "one_hot = pd.get_dummies(X_test[\"location\"]).astype(int)\n",
    "X_test = X_test.drop(\"location\", axis=1)\n",
    "X_test = pd.merge(X_test, one_hot, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_exclude = [\"A\", \"B\", \"C\", \"dew_or_rime:idx\", \"is_day:idx\", \"_in_shadow:idx\"]\n",
    "\n",
    "columns_to_normalize = [col for col in X_train.columns if col not in columns_to_exclude]\n",
    "\n",
    "#Min-max\n",
    "# Calculate min and max values for scaling\n",
    "X_min = X_train[columns_to_normalize].min()\n",
    "X_max = X_train[columns_to_normalize].max()\n",
    "\n",
    "# Apply min-max scaling to the columns to be normalized\n",
    "X_train[columns_to_normalize] = (X_train[columns_to_normalize] - X_min) / (X_max - X_min)\n",
    "X_validate[columns_to_normalize] = (X_validate[columns_to_normalize] - X_min) / (X_max - X_min)\n",
    "X_test[columns_to_normalize] = (X_test[columns_to_normalize] - X_min) / (X_max - X_min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y_train[\"pv_measurement\"] = y_scaler.fit_transform(y_train[\"pv_measurement\"].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index().drop(columns=\"index\")\n",
    "y_train = y_train.reset_index().drop(columns=\"index\")\n",
    "new_train = pd.merge(X_train, y_train[\"pv_measurement\"], left_index=True, right_index=True)\n",
    "new_train = new_train.fillna(0)\n",
    "\n",
    "X_validate = X_validate.reset_index().drop(columns=\"index\")\n",
    "y_validate = y_validate.reset_index().drop(columns=\"index\")\n",
    "new_validate = pd.merge(X_validate, y_validate[\"pv_measurement\"], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train.to_csv(\"../data/processed/train.csv\", index=False)\n",
    "X_test.to_csv(\"../data/processed/X_test.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
