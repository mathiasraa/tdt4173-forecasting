{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan:\n",
    " - Load sets\n",
    " - Clean:\n",
    "    - remove duplicate entries from sets\n",
    "    - Drop the unimportant coloumns that we found\n",
    "    - Make querters into hours\n",
    "    - remove values from train_targets that are nan in both x and y sets\n",
    "    - make time features\n",
    "    - drop datetime coloumns\n",
    " - Set prep:\n",
    "    - 75/25 fordeling. 75% av OBSERVED er trening, resterende 25% er validation, 75% av ESTIMATED er trening, resterende 25% er validation\n",
    "- Concat the different locations into 1 set, where location is a feature as well\n",
    "   - 1 hot encoding\n",
    "- Make a time series model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A\n",
    "y_a = pd.read_parquet('../data/A/train_targets.parquet')\n",
    "X_test_estimated_a = pd.read_parquet('../data/A/X_test_estimated.parquet')\n",
    "X_train_estimated_a = pd.read_parquet('../data/A/X_train_estimated.parquet')\n",
    "X_train_observed_a = pd.read_parquet('../data/A/X_train_observed.parquet')\n",
    "\n",
    "# B\n",
    "y_b = pd.read_parquet('../data/B/train_targets.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('../data/B/X_test_estimated.parquet')\n",
    "X_train_estimated_b = pd.read_parquet('../data/B/X_train_estimated.parquet')\n",
    "X_train_observed_b = pd.read_parquet('../data/B/X_train_observed.parquet')\n",
    "\n",
    "# C\n",
    "y_c = pd.read_parquet('../data/C/train_targets.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('../data/C/X_test_estimated.parquet')\n",
    "X_train_estimated_c = pd.read_parquet('../data/C/X_train_estimated.parquet')\n",
    "X_train_observed_c = pd.read_parquet('../data/C/X_train_observed.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing duplicate entries from the sets if any exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that removes the duplicates if it finds duplicates in the specified coloumn\n",
    "def remove_duplicates_in_coloumn(df, col):\n",
    "    duplicate_mask = df[col].duplicated(keep=\"first\")\n",
    "    if duplicate_mask.any():\n",
    "        df = df[~duplicate_mask]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A\n",
    "y_a = remove_duplicates_in_coloumn(y_a, \"time\")\n",
    "X_test_estimated_a = remove_duplicates_in_coloumn(X_test_estimated_a, \"date_forecast\")\n",
    "X_train_estimated_a = remove_duplicates_in_coloumn(X_train_estimated_a, \"date_forecast\")\n",
    "X_train_observed_a = remove_duplicates_in_coloumn(X_train_observed_a, \"date_forecast\")\n",
    "\n",
    "#B\n",
    "y_b = remove_duplicates_in_coloumn(y_b, \"time\")\n",
    "X_test_estimated_b = remove_duplicates_in_coloumn(X_test_estimated_b, \"date_forecast\")\n",
    "X_train_estimated_b = remove_duplicates_in_coloumn(X_train_estimated_b, \"date_forecast\")\n",
    "X_train_observed_b = remove_duplicates_in_coloumn(X_train_observed_b, \"date_forecast\")\n",
    "\n",
    "#C\n",
    "y_c = remove_duplicates_in_coloumn(y_c, \"time\")\n",
    "X_test_estimated_c = remove_duplicates_in_coloumn(X_test_estimated_c, \"date_forecast\")\n",
    "X_train_estimated_c = remove_duplicates_in_coloumn(X_train_estimated_c, \"date_forecast\")\n",
    "X_train_observed_c = remove_duplicates_in_coloumn(X_train_observed_c, \"date_forecast\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of all estimated and observed sets\n",
    "\n",
    "list_of_all_estimated_and_observed_sets = [X_test_estimated_a, X_train_estimated_a, X_train_observed_a,\n",
    "                                           X_test_estimated_b, X_train_estimated_b, X_train_observed_b,\n",
    "                                           X_test_estimated_c, X_train_estimated_c, X_train_observed_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping some coloumns for the bants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for set in list_of_all_estimated_and_observed_sets:\n",
    "#     # set.drop(\"snow_density:kgm3\", axis=1, inplace=True)\n",
    "#     # these 2 had a lot of NaN values\n",
    "#     set.drop(\"ceiling_height_agl:m\", axis=1, inplace=True)\n",
    "#     set.drop(\"cloud_base_agl:m\", axis=1,inplace=True) # could potentially not drop this, but set all nan values to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting every 4 quarters into an whole hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "## It might be better not to resample actually, but just use the actual hours that correspond perfectly\n",
    "\n",
    "def convert_df_into_hourly(df):\n",
    "    df.set_index(\"date_forecast\", inplace=True)\n",
    "    df = df.resample('1H').mean()\n",
    "    df.reset_index(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_estimated_a = convert_df_into_hourly(X_train_estimated_a)\n",
    "X_train_observed_a = convert_df_into_hourly(X_train_observed_a)\n",
    "X_test_estimated_a = convert_df_into_hourly(X_test_estimated_a)\n",
    "X_train_estimated_b = convert_df_into_hourly(X_train_estimated_b)\n",
    "X_train_observed_b = convert_df_into_hourly(X_train_observed_b)\n",
    "X_test_estimated_b = convert_df_into_hourly(X_test_estimated_b)\n",
    "X_train_estimated_c = convert_df_into_hourly(X_train_estimated_c)\n",
    "X_train_observed_c = convert_df_into_hourly(X_train_observed_c)\n",
    "X_test_estimated_c = convert_df_into_hourly(X_test_estimated_c)\n",
    "\n",
    "## We should probably do it for the test_sets here as well\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing date_calc from all estimated sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_estimated_a.drop(\"date_calc\", axis=1, inplace=True)\n",
    "X_train_estimated_a.drop(\"date_calc\", axis=1, inplace=True)\n",
    "X_test_estimated_b.drop(\"date_calc\", axis=1, inplace=True)\n",
    "X_train_estimated_b.drop(\"date_calc\", axis=1, inplace=True)\n",
    "X_test_estimated_c.drop(\"date_calc\", axis=1, inplace=True)\n",
    "X_train_estimated_c.drop(\"date_calc\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing all NaN from y_targets\n",
    " - Removing all NaN from y_targets and their corresponding dates in the X_train sets, both observed and estimated\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that returns train_targets, observed and estimated sets left after filtering away NaN\n",
    "def drop_nan_rows_in_target_and_train(y_df, observed_train_df, estimated_train_df):\n",
    "    y_df = y_df.dropna(subset=['pv_measurement'])\n",
    "    valid_dates = y_df['time']\n",
    "    observed_train_df = observed_train_df[observed_train_df['date_forecast'].isin(valid_dates)]\n",
    "    estimated_train_df = estimated_train_df[estimated_train_df['date_forecast'].isin(valid_dates)]\n",
    "    return (y_df, observed_train_df, estimated_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>pv_measurement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5913</th>\n",
       "      <td>2019-09-04 08:00:00</td>\n",
       "      <td>137.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5914</th>\n",
       "      <td>2019-09-04 09:00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5915</th>\n",
       "      <td>2019-09-04 10:00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5916</th>\n",
       "      <td>2019-09-04 11:00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5917</th>\n",
       "      <td>2019-09-04 12:00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    time  pv_measurement\n",
       "5913 2019-09-04 08:00:00           137.2\n",
       "5914 2019-09-04 09:00:00             0.0\n",
       "5915 2019-09-04 10:00:00             0.0\n",
       "5916 2019-09-04 11:00:00             0.0\n",
       "5917 2019-09-04 12:00:00             0.0"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#doing the NaN filtering\n",
    "y_a, X_train_observed_a, X_train_estimated_a = drop_nan_rows_in_target_and_train(y_a, X_train_observed_a, X_train_estimated_a)\n",
    "y_b, X_train_observed_b, X_train_estimated_b = drop_nan_rows_in_target_and_train(y_b, X_train_observed_b, X_train_estimated_b)\n",
    "y_c, X_train_observed_c, X_train_estimated_c = drop_nan_rows_in_target_and_train(y_c, X_train_observed_c, X_train_estimated_c)\n",
    "\n",
    "y_c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_forecast</th>\n",
       "      <th>absolute_humidity_2m:gm3</th>\n",
       "      <th>air_density_2m:kgm3</th>\n",
       "      <th>ceiling_height_agl:m</th>\n",
       "      <th>clear_sky_energy_1h:J</th>\n",
       "      <th>clear_sky_rad:W</th>\n",
       "      <th>cloud_base_agl:m</th>\n",
       "      <th>dew_or_rime:idx</th>\n",
       "      <th>dew_point_2m:K</th>\n",
       "      <th>diffuse_rad:W</th>\n",
       "      <th>...</th>\n",
       "      <th>sun_azimuth:d</th>\n",
       "      <th>sun_elevation:d</th>\n",
       "      <th>super_cooled_liquid_water:kgm2</th>\n",
       "      <th>t_1000hPa:K</th>\n",
       "      <th>total_cloud_cover:p</th>\n",
       "      <th>visibility:m</th>\n",
       "      <th>wind_speed_10m:ms</th>\n",
       "      <th>wind_speed_u_10m:ms</th>\n",
       "      <th>wind_speed_v_10m:ms</th>\n",
       "      <th>wind_speed_w_1000hPa:ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5912</th>\n",
       "      <td>2019-09-04 08:00:00</td>\n",
       "      <td>6.625</td>\n",
       "      <td>1.22075</td>\n",
       "      <td>2287.250000</td>\n",
       "      <td>1320844.375</td>\n",
       "      <td>421.225006</td>\n",
       "      <td>2287.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>278.200012</td>\n",
       "      <td>67.974998</td>\n",
       "      <td>...</td>\n",
       "      <td>130.353500</td>\n",
       "      <td>25.754999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>283.725006</td>\n",
       "      <td>31.199999</td>\n",
       "      <td>49549.449219</td>\n",
       "      <td>1.250</td>\n",
       "      <td>-0.825</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5913</th>\n",
       "      <td>2019-09-04 09:00:00</td>\n",
       "      <td>6.275</td>\n",
       "      <td>1.21425</td>\n",
       "      <td>2679.074951</td>\n",
       "      <td>1681730.500</td>\n",
       "      <td>508.125000</td>\n",
       "      <td>2679.074951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>277.424988</td>\n",
       "      <td>76.625000</td>\n",
       "      <td>...</td>\n",
       "      <td>146.494507</td>\n",
       "      <td>30.204250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>284.799988</td>\n",
       "      <td>86.250000</td>\n",
       "      <td>52309.976562</td>\n",
       "      <td>1.500</td>\n",
       "      <td>-1.500</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5914</th>\n",
       "      <td>2019-09-04 10:00:00</td>\n",
       "      <td>5.900</td>\n",
       "      <td>1.20825</td>\n",
       "      <td>2983.750000</td>\n",
       "      <td>1936799.875</td>\n",
       "      <td>561.875000</td>\n",
       "      <td>2983.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>276.650024</td>\n",
       "      <td>112.574997</td>\n",
       "      <td>...</td>\n",
       "      <td>163.708252</td>\n",
       "      <td>33.027500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>285.899994</td>\n",
       "      <td>95.149994</td>\n",
       "      <td>53978.800781</td>\n",
       "      <td>2.000</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5915</th>\n",
       "      <td>2019-09-04 11:00:00</td>\n",
       "      <td>5.875</td>\n",
       "      <td>1.20350</td>\n",
       "      <td>3286.550049</td>\n",
       "      <td>2063526.250</td>\n",
       "      <td>578.025024</td>\n",
       "      <td>3286.550049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>276.649994</td>\n",
       "      <td>195.149994</td>\n",
       "      <td>...</td>\n",
       "      <td>181.566742</td>\n",
       "      <td>33.887001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>286.924988</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>54510.925781</td>\n",
       "      <td>1.950</td>\n",
       "      <td>-1.950</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5916</th>\n",
       "      <td>2019-09-04 12:00:00</td>\n",
       "      <td>6.150</td>\n",
       "      <td>1.20175</td>\n",
       "      <td>3453.425049</td>\n",
       "      <td>2051320.000</td>\n",
       "      <td>555.174988</td>\n",
       "      <td>3453.425049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>277.250000</td>\n",
       "      <td>243.375000</td>\n",
       "      <td>...</td>\n",
       "      <td>199.359009</td>\n",
       "      <td>32.668999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>287.500000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>54284.648438</td>\n",
       "      <td>1.525</td>\n",
       "      <td>-1.500</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date_forecast  absolute_humidity_2m:gm3  air_density_2m:kgm3  \\\n",
       "5912 2019-09-04 08:00:00                     6.625              1.22075   \n",
       "5913 2019-09-04 09:00:00                     6.275              1.21425   \n",
       "5914 2019-09-04 10:00:00                     5.900              1.20825   \n",
       "5915 2019-09-04 11:00:00                     5.875              1.20350   \n",
       "5916 2019-09-04 12:00:00                     6.150              1.20175   \n",
       "\n",
       "      ceiling_height_agl:m  clear_sky_energy_1h:J  clear_sky_rad:W  \\\n",
       "5912           2287.250000            1320844.375       421.225006   \n",
       "5913           2679.074951            1681730.500       508.125000   \n",
       "5914           2983.750000            1936799.875       561.875000   \n",
       "5915           3286.550049            2063526.250       578.025024   \n",
       "5916           3453.425049            2051320.000       555.174988   \n",
       "\n",
       "      cloud_base_agl:m  dew_or_rime:idx  dew_point_2m:K  diffuse_rad:W  ...  \\\n",
       "5912       2287.250000              0.0      278.200012      67.974998  ...   \n",
       "5913       2679.074951              0.0      277.424988      76.625000  ...   \n",
       "5914       2983.750000              0.0      276.650024     112.574997  ...   \n",
       "5915       3286.550049              0.0      276.649994     195.149994  ...   \n",
       "5916       3453.425049              0.0      277.250000     243.375000  ...   \n",
       "\n",
       "      sun_azimuth:d  sun_elevation:d  super_cooled_liquid_water:kgm2  \\\n",
       "5912     130.353500        25.754999                             0.0   \n",
       "5913     146.494507        30.204250                             0.0   \n",
       "5914     163.708252        33.027500                             0.0   \n",
       "5915     181.566742        33.887001                             0.0   \n",
       "5916     199.359009        32.668999                             0.0   \n",
       "\n",
       "      t_1000hPa:K  total_cloud_cover:p  visibility:m  wind_speed_10m:ms  \\\n",
       "5912   283.725006            31.199999  49549.449219              1.250   \n",
       "5913   284.799988            86.250000  52309.976562              1.500   \n",
       "5914   285.899994            95.149994  53978.800781              2.000   \n",
       "5915   286.924988           100.000000  54510.925781              1.950   \n",
       "5916   287.500000           100.000000  54284.648438              1.525   \n",
       "\n",
       "      wind_speed_u_10m:ms  wind_speed_v_10m:ms  wind_speed_w_1000hPa:ms  \n",
       "5912               -0.825                0.825                      0.0  \n",
       "5913               -1.500               -0.200                      0.0  \n",
       "5914               -2.000               -0.225                      0.0  \n",
       "5915               -1.950               -0.250                      0.0  \n",
       "5916               -1.500               -0.300                      0.0  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_observed_c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that creates timefeatures based on the specified coloumn\n",
    "def create_time_features_based_on_coloun(df, col):   \n",
    "    df['hour'] = df[col].dt.hour\n",
    "    df['dayofmonth'] = df[col].dt.day\n",
    "    df['dayofweek'] = df[col].dt.dayofweek\n",
    "    df['quarter'] = df[col].dt.quarter\n",
    "    df['month'] = df[col].dt.month\n",
    "    df['year'] = df[col].dt.year\n",
    "    df['dayofyear'] = df[col].dt.dayofyear\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding time features\n",
    "\n",
    "#A\n",
    "X_train_estimated_a = create_time_features_based_on_coloun(X_train_estimated_a, \"date_forecast\")\n",
    "X_train_observed_a = create_time_features_based_on_coloun(X_train_observed_a, \"date_forecast\")\n",
    "X_test_estimated_a = create_time_features_based_on_coloun(X_test_estimated_a, \"date_forecast\")\n",
    "\n",
    "#B\n",
    "X_train_estimated_b = create_time_features_based_on_coloun(X_train_estimated_b, \"date_forecast\")\n",
    "X_train_observed_b = create_time_features_based_on_coloun(X_train_observed_b, \"date_forecast\")\n",
    "X_test_estimated_b = create_time_features_based_on_coloun(X_test_estimated_b, \"date_forecast\")\n",
    "\n",
    "#C\n",
    "X_train_estimated_c = create_time_features_based_on_coloun(X_train_estimated_c, \"date_forecast\")\n",
    "X_train_observed_c = create_time_features_based_on_coloun(X_train_observed_c, \"date_forecast\")\n",
    "X_test_estimated_c = create_time_features_based_on_coloun(X_test_estimated_c, \"date_forecast\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just showing the shapes to se if i havent f'ed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "(4418, 53)\n",
      "(29667, 53)\n",
      "34085\n",
      "(34085, 2)\n",
      "B\n",
      "(3625, 53)\n",
      "(29218, 53)\n",
      "32843\n",
      "(32844, 2)\n",
      "C\n",
      "(2954, 53)\n",
      "(23141, 53)\n",
      "26095\n",
      "(26095, 2)\n"
     ]
    }
   ],
   "source": [
    "# printing shapes\n",
    "print(\"A\")\n",
    "print(X_train_estimated_a.shape)\n",
    "print(X_train_observed_a.shape)\n",
    "print(X_train_estimated_a.shape[0]+X_train_observed_a.shape[0])\n",
    "print(y_a.shape)\n",
    "\n",
    "print(\"B\")\n",
    "print(X_train_estimated_b.shape)\n",
    "print(X_train_observed_b.shape)\n",
    "print(X_train_estimated_b.shape[0]+X_train_observed_b.shape[0])  ## this one seems to be of by 1??????\n",
    "print(y_b.shape)\n",
    "\n",
    "print(\"C\")\n",
    "print(X_train_estimated_c.shape)\n",
    "print(X_train_observed_c.shape)\n",
    "print(X_train_estimated_c.shape[0]+X_train_observed_c.shape[0])\n",
    "print(y_c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for B there is actually on row more in the target set for some reason, finding that row and deleting it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 time  pv_measurement\n",
      "0 2018-12-31 23:00:00             0.0\n"
     ]
    }
   ],
   "source": [
    "## Finding the row in y_b that does not have a match\n",
    "\n",
    "# renaming\n",
    "date_times_estimated = X_train_estimated_b['date_forecast']\n",
    "date_times_observed = X_train_observed_b['date_forecast']\n",
    "result_df = y_b[~y_b['time'].isin(date_times_estimated) & ~y_b['time'].isin(date_times_observed)]\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okey so let's remove that row and verify that everything is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n",
      "(3625, 53)\n",
      "(29218, 53)\n",
      "32843\n",
      "(32843, 2)\n"
     ]
    }
   ],
   "source": [
    "# removing row\n",
    "y_b = y_b[~y_b['time'].isin(result_df['time'])]\n",
    "\n",
    "# checking that the numbers add up again\n",
    "print(\"B\")\n",
    "print(X_train_estimated_b.shape)\n",
    "print(X_train_observed_b.shape)\n",
    "print(X_train_estimated_b.shape[0]+X_train_observed_b.shape[0])\n",
    "print(y_b.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing repeated indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dividing the sets into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_forecast</th>\n",
       "      <th>absolute_humidity_2m:gm3</th>\n",
       "      <th>air_density_2m:kgm3</th>\n",
       "      <th>ceiling_height_agl:m</th>\n",
       "      <th>clear_sky_energy_1h:J</th>\n",
       "      <th>clear_sky_rad:W</th>\n",
       "      <th>cloud_base_agl:m</th>\n",
       "      <th>dew_or_rime:idx</th>\n",
       "      <th>dew_point_2m:K</th>\n",
       "      <th>diffuse_rad:W</th>\n",
       "      <th>...</th>\n",
       "      <th>wind_speed_u_10m:ms</th>\n",
       "      <th>wind_speed_v_10m:ms</th>\n",
       "      <th>wind_speed_w_1000hPa:ms</th>\n",
       "      <th>hour</th>\n",
       "      <th>dayofmonth</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>dayofyear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-06-02 22:00:00</td>\n",
       "      <td>7.700</td>\n",
       "      <td>1.22825</td>\n",
       "      <td>1728.949951</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1728.949951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>280.299988</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.575</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2019</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-06-02 23:00:00</td>\n",
       "      <td>7.700</td>\n",
       "      <td>1.22350</td>\n",
       "      <td>1689.824951</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1689.824951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>280.299988</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.350</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2019</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-06-03 00:00:00</td>\n",
       "      <td>7.875</td>\n",
       "      <td>1.21975</td>\n",
       "      <td>1563.224976</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1563.224976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>280.649994</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.950</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2019</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-06-03 01:00:00</td>\n",
       "      <td>8.425</td>\n",
       "      <td>1.21800</td>\n",
       "      <td>1283.425049</td>\n",
       "      <td>208.649994</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1283.425049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>281.674988</td>\n",
       "      <td>0.300</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2019</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-06-03 02:00:00</td>\n",
       "      <td>8.950</td>\n",
       "      <td>1.21800</td>\n",
       "      <td>1003.500000</td>\n",
       "      <td>32468.150391</td>\n",
       "      <td>23.10</td>\n",
       "      <td>1003.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>282.500000</td>\n",
       "      <td>11.975</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.350</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2019</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date_forecast  absolute_humidity_2m:gm3  air_density_2m:kgm3  \\\n",
       "0 2019-06-02 22:00:00                     7.700              1.22825   \n",
       "1 2019-06-02 23:00:00                     7.700              1.22350   \n",
       "2 2019-06-03 00:00:00                     7.875              1.21975   \n",
       "3 2019-06-03 01:00:00                     8.425              1.21800   \n",
       "4 2019-06-03 02:00:00                     8.950              1.21800   \n",
       "\n",
       "   ceiling_height_agl:m  clear_sky_energy_1h:J  clear_sky_rad:W  \\\n",
       "0           1728.949951               0.000000             0.00   \n",
       "1           1689.824951               0.000000             0.00   \n",
       "2           1563.224976               0.000000             0.00   \n",
       "3           1283.425049             208.649994             0.75   \n",
       "4           1003.500000           32468.150391            23.10   \n",
       "\n",
       "   cloud_base_agl:m  dew_or_rime:idx  dew_point_2m:K  diffuse_rad:W  ...  \\\n",
       "0       1728.949951              0.0      280.299988          0.000  ...   \n",
       "1       1689.824951              0.0      280.299988          0.000  ...   \n",
       "2       1563.224976              0.0      280.649994          0.000  ...   \n",
       "3       1283.425049              0.0      281.674988          0.300  ...   \n",
       "4       1003.500000              0.0      282.500000         11.975  ...   \n",
       "\n",
       "   wind_speed_u_10m:ms  wind_speed_v_10m:ms  wind_speed_w_1000hPa:ms  hour  \\\n",
       "0               -3.575               -0.500                      0.0    22   \n",
       "1               -3.350                0.275                      0.0    23   \n",
       "2               -2.950                0.750                      0.0     0   \n",
       "3               -2.600                0.875                      0.0     1   \n",
       "4               -2.350                0.925                      0.0     2   \n",
       "\n",
       "   dayofmonth  dayofweek  quarter  month  year  dayofyear  \n",
       "0           2          6        2      6  2019        153  \n",
       "1           2          6        2      6  2019        153  \n",
       "2           3          0        2      6  2019        154  \n",
       "3           3          0        2      6  2019        154  \n",
       "4           3          0        2      6  2019        154  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first renaming the date_forecast columns to time  (DOES NOT WORK)\n",
    "for df in list_of_all_estimated_and_observed_sets:               \n",
    "    df.rename(columns={'date_forecast': 'time'}, inplace=True)\n",
    "\n",
    "#maybe we can drop it here already actually\n",
    "X_train_observed_a.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data = first 75% of observed + first 75% of estimated\n",
    "Validation data = last 25% of observed + last 25% of estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23858, 53) (23858, 2)\n",
      "(22989, 53) (22989, 2)\n",
      "(18265, 53) (18265, 2)\n"
     ]
    }
   ],
   "source": [
    "# Making training and validation data for A\n",
    "percent_observed_train = 0.7\n",
    "percent_estimated_train = 0.7\n",
    "\n",
    "split_index_obs_a = int(len(X_train_observed_a)*percent_observed_train)\n",
    "X_train_observed_a_first_75 = X_train_observed_a[:split_index_obs_a]\n",
    "X_train_observed_a_last_25 = X_train_observed_a[split_index_obs_a:]\n",
    "\n",
    "split_index_est_a = int(len(X_train_estimated_a)*percent_estimated_train)\n",
    "X_train_estimated_a_first_75 = X_train_estimated_a[:split_index_est_a]\n",
    "X_train_estimated_a_last_25 = X_train_estimated_a[split_index_est_a:]\n",
    "\n",
    "X_train_a = pd.concat([X_train_observed_a_first_75, X_train_estimated_a_first_75])\n",
    "y_train_a = y_a[y_a[\"time\"].isin(X_train_a['date_forecast'])]\n",
    "print(X_train_a.shape, y_train_a.shape)\n",
    "\n",
    "X_validate_a = pd.concat([X_train_observed_a_last_25, X_train_estimated_a_last_25])\n",
    "y_validate_a = y_a[y_a[\"time\"].isin(X_validate_a['date_forecast'])]\n",
    "\n",
    "\n",
    "# making training and validation for B\n",
    "split_index_obs_b = int(len(X_train_observed_b)*percent_observed_train)\n",
    "X_train_observed_b_first_75 = X_train_observed_b[:split_index_obs_b]\n",
    "X_train_observed_b_last_25 = X_train_observed_b[split_index_obs_b:]\n",
    "\n",
    "\n",
    "split_index_est_b = int(len(X_train_estimated_b)*percent_estimated_train)\n",
    "X_train_estimated_b_first_75 = X_train_estimated_b[:split_index_est_b]\n",
    "X_train_estimated_b_last_25 = X_train_estimated_b[split_index_est_b:]\n",
    "\n",
    "X_train_b = pd.concat([X_train_observed_b_first_75, X_train_estimated_b_first_75])\n",
    "y_train_b = y_b[y_b[\"time\"].isin(X_train_b['date_forecast'])]\n",
    "print(X_train_b.shape, y_train_b.shape)\n",
    "\n",
    "X_validate_b = pd.concat([X_train_observed_b_last_25, X_train_estimated_b_last_25])\n",
    "y_validate_b = y_b[y_b[\"time\"].isin(X_validate_b['date_forecast'])]\n",
    "\n",
    "# making training and validation for C\n",
    "split_index_obs_c = int(len(X_train_observed_c)*percent_observed_train)\n",
    "X_train_observed_c_first_75 = X_train_observed_c[:split_index_obs_c]\n",
    "X_train_observed_c_last_25 = X_train_observed_c[split_index_obs_c:]\n",
    "\n",
    "split_index_est_c = int(len(X_train_estimated_c)*percent_estimated_train)\n",
    "X_train_estimated_c_first_75 = X_train_estimated_c[:split_index_est_c]\n",
    "X_train_estimated_c_last_25 = X_train_estimated_c[split_index_est_c:]\n",
    "\n",
    "X_train_c = pd.concat([X_train_observed_c_first_75, X_train_estimated_c_first_75])\n",
    "y_train_c = y_c[y_c[\"time\"].isin(X_train_c['date_forecast'])]\n",
    "print(X_train_c.shape, y_train_c.shape)\n",
    "\n",
    "X_validate_c = pd.concat([X_train_observed_c_last_25, X_train_estimated_c_last_25])\n",
    "y_validate_c = y_c[y_c[\"time\"].isin(X_validate_c['date_forecast'])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing repeated indexes here, think this might be the wrong place to do it, should probably to it for the concat BUT hEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23858, 53)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23816, 54)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# def find_repeated_indexes(df, column_name, repeat_count=24):\n",
    "#     \"\"\"\n",
    "#     Find and return the indexes of rows with a specified number of repeated values in a given column.\n",
    "\n",
    "#     Parameters:\n",
    "#     - df: DataFrame to search for repeated rows.\n",
    "#     - column_name: Name of the column to check for repeated values.\n",
    "#     - repeat_count: Number of repeated values required to consider a row as a match.\n",
    "\n",
    "#     Returns:\n",
    "#     - List of indexes for rows with the specified number of repeated values in the given column.\n",
    "#     \"\"\"\n",
    "#     df = df.reset_index()\n",
    "#     repeated_indexes = []\n",
    "#     temp_repeated_indexes = []\n",
    "#     current_value = None\n",
    "#     count = 0\n",
    "\n",
    "#     for index, row in df.iterrows():\n",
    "\n",
    "#         value = row[column_name]\n",
    "\n",
    "#         if value == current_value:\n",
    "#             count += 1\n",
    "#             temp_repeated_indexes.append(index)\n",
    "#         else:\n",
    "#             count = 1\n",
    "#             current_value = value\n",
    "#             temp_repeated_indexes = []\n",
    "\n",
    "#         if count >= repeat_count:\n",
    "#             for i in temp_repeated_indexes:\n",
    "#                 if i not in repeated_indexes:\n",
    "#                     repeated_indexes.append(i)\n",
    "\n",
    "#     return repeated_indexes\n",
    "\n",
    "\n",
    "# print(X_train_a.shape)\n",
    "# X_train_a = X_train_a.reset_index()\n",
    "# repeated_indices = find_repeated_indexes(y_train_a,\"pv_measurement\", 24)\n",
    "# X_train_a = X_train_a.drop(index=repeated_indices)\n",
    "# y_train_a = y_train_a.drop(index=repeated_indices)\n",
    "# print(X_train_a.shape)\n",
    "\n",
    "# repeated_indices = find_repeated_indexes(y_validate_a,\"pv_measurement\",24)\n",
    "# X_validate_a = X_validate_a.reset_index()\n",
    "# X_validate_a = X_validate_a.drop(index=repeated_indices)\n",
    "# y_validate_a = y_train_a.drop(index=repeated_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing time feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siver\\AppData\\Local\\Temp\\ipykernel_26408\\3483724575.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_train_b.drop(\"time\", axis=1, inplace=True)\n",
      "C:\\Users\\siver\\AppData\\Local\\Temp\\ipykernel_26408\\3483724575.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_validate_b.drop(\"time\", axis=1, inplace=True)\n",
      "C:\\Users\\siver\\AppData\\Local\\Temp\\ipykernel_26408\\3483724575.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_train_c.drop(\"time\", axis=1, inplace=True)\n",
      "C:\\Users\\siver\\AppData\\Local\\Temp\\ipykernel_26408\\3483724575.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_validate_c.drop(\"time\", axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "X_train_a.drop(\"date_forecast\", axis=1, inplace=True)\n",
    "y_train_a.drop(\"time\", axis=1, inplace=True)\n",
    "X_validate_a.drop(\"date_forecast\", axis=1, inplace=True)\n",
    "y_validate_a.drop(\"time\", axis=1, inplace=True)\n",
    "\n",
    "X_train_b.drop(\"date_forecast\", axis=1, inplace=True)\n",
    "y_train_b.drop(\"time\", axis=1, inplace=True)\n",
    "X_validate_b.drop(\"date_forecast\", axis=1, inplace=True)\n",
    "y_validate_b.drop(\"time\", axis=1, inplace=True)\n",
    "\n",
    "X_train_c.drop(\"date_forecast\", axis=1, inplace=True)\n",
    "y_train_c.drop(\"time\", axis=1, inplace=True)\n",
    "X_validate_c.drop(\"date_forecast\", axis=1, inplace=True)\n",
    "y_validate_c.drop(\"time\", axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding location features to the sets before merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siver\\AppData\\Local\\Temp\\ipykernel_26408\\816523439.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_train_b[\"location\"] = 2\n",
      "C:\\Users\\siver\\AppData\\Local\\Temp\\ipykernel_26408\\816523439.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_validate_b[\"location\"] = 2\n",
      "C:\\Users\\siver\\AppData\\Local\\Temp\\ipykernel_26408\\816523439.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_train_c[\"location\"] = 3\n",
      "C:\\Users\\siver\\AppData\\Local\\Temp\\ipykernel_26408\\816523439.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_validate_c[\"location\"] = 3\n"
     ]
    }
   ],
   "source": [
    "X_train_a[\"location\"] =  1 \n",
    "y_train_a[\"location\"] = 1\n",
    "X_validate_a[\"location\"] = 1\n",
    "y_validate_a[\"location\"] = 1\n",
    "\n",
    "X_train_b[\"location\"] = 2\n",
    "y_train_b[\"location\"] = 2\n",
    "X_validate_b[\"location\"] = 2\n",
    "y_validate_b[\"location\"] = 2\n",
    "\n",
    "X_train_c[\"location\"] = 3\n",
    "y_train_c[\"location\"] = 3\n",
    "X_validate_c[\"location\"] = 3\n",
    "y_validate_c[\"location\"] = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging tranining data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65070, 54)\n",
      "(65070, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.concat([X_train_a, X_train_b, X_train_c])\n",
    "y_train = pd.concat([y_train_a, y_train_b, y_train_c])\n",
    "\n",
    "X_validate = pd.concat([X_validate_a, X_validate_b, X_validate_c])\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding and dropping location feature   some fuckery here\n",
    "# one_hot = pd.get_dummies(X_train[\"location\"]).astype(int)\n",
    "# X_train = X_train.drop(\"location\", axis=1)\n",
    "# X_train = pd.merge(X_train, one_hot, left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "# one_hot = pd.get_dummies(X_validate[\"location\"]).astype(int)\n",
    "# X_validate = pd.merge(X_validate, one_hot, left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test an XG boost only on A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "c:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n",
      "c:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype):\n",
      "c:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "c:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "c:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n",
      "c:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype):\n",
      "c:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "c:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "c:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n",
      "c:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype):\n",
      "c:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "feature_names mismatch: ['index', 'absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', 'cloud_base_agl:m', 'dew_or_rime:idx', 'dew_point_2m:K', 'diffuse_rad:W', 'diffuse_rad_1h:J', 'direct_rad:W', 'direct_rad_1h:J', 'effective_cloud_cover:p', 'elevation:m', 'fresh_snow_12h:cm', 'fresh_snow_1h:cm', 'fresh_snow_24h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'is_day:idx', 'is_in_shadow:idx', 'msl_pressure:hPa', 'precip_5min:mm', 'precip_type_5min:idx', 'pressure_100m:hPa', 'pressure_50m:hPa', 'prob_rime:p', 'rain_water:kgm2', 'relative_humidity_1000hPa:p', 'sfc_pressure:hPa', 'snow_density:kgm3', 'snow_depth:cm', 'snow_drift:idx', 'snow_melt_10min:mm', 'snow_water:kgm2', 'sun_azimuth:d', 'sun_elevation:d', 'super_cooled_liquid_water:kgm2', 't_1000hPa:K', 'total_cloud_cover:p', 'visibility:m', 'wind_speed_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms', 'wind_speed_w_1000hPa:ms', 'hour', 'dayofmonth', 'dayofweek', 'quarter', 'month', 'year', 'dayofyear', 'location'] ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', 'cloud_base_agl:m', 'dew_or_rime:idx', 'dew_point_2m:K', 'diffuse_rad:W', 'diffuse_rad_1h:J', 'direct_rad:W', 'direct_rad_1h:J', 'effective_cloud_cover:p', 'elevation:m', 'fresh_snow_12h:cm', 'fresh_snow_1h:cm', 'fresh_snow_24h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'is_day:idx', 'is_in_shadow:idx', 'msl_pressure:hPa', 'precip_5min:mm', 'precip_type_5min:idx', 'pressure_100m:hPa', 'pressure_50m:hPa', 'prob_rime:p', 'rain_water:kgm2', 'relative_humidity_1000hPa:p', 'sfc_pressure:hPa', 'snow_density:kgm3', 'snow_depth:cm', 'snow_drift:idx', 'snow_melt_10min:mm', 'snow_water:kgm2', 'sun_azimuth:d', 'sun_elevation:d', 'super_cooled_liquid_water:kgm2', 't_1000hPa:K', 'total_cloud_cover:p', 'visibility:m', 'wind_speed_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms', 'wind_speed_w_1000hPa:ms', 'hour', 'dayofmonth', 'dayofweek', 'quarter', 'month', 'year', 'dayofyear', 'location']\nexpected index in input data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\notebooks\\siverts_super_model.ipynb Cell 45\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/siver/NTNU%20H%C3%B8st%202023/Machine%20Learning/Group%20Project/tdt4173-forecasting/notebooks/siverts_super_model.ipynb#X60sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# X_validate_a_loc = X_validate[X_validate_a[\"location\"]==\"A\"]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/siver/NTNU%20H%C3%B8st%202023/Machine%20Learning/Group%20Project/tdt4173-forecasting/notebooks/siverts_super_model.ipynb#X60sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# X_validate_a_loc = X_validate_a_loc.drop(\"location\", axis=1)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/siver/NTNU%20H%C3%B8st%202023/Machine%20Learning/Group%20Project/tdt4173-forecasting/notebooks/siverts_super_model.ipynb#X60sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m prediction_a \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_validate_a)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/siver/NTNU%20H%C3%B8st%202023/Machine%20Learning/Group%20Project/tdt4173-forecasting/notebooks/siverts_super_model.ipynb#X60sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m prediction_b \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(X_validate_b)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/siver/NTNU%20H%C3%B8st%202023/Machine%20Learning/Group%20Project/tdt4173-forecasting/notebooks/siverts_super_model.ipynb#X60sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m prediction_c \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_validate_c)\n",
      "File \u001b[1;32mc:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\sklearn.py:1164\u001b[0m, in \u001b[0;36mXGBModel.predict\u001b[1;34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m   1162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_can_use_inplace_predict():\n\u001b[0;32m   1163\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1164\u001b[0m         predts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_booster()\u001b[39m.\u001b[39;49minplace_predict(\n\u001b[0;32m   1165\u001b[0m             data\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m   1166\u001b[0m             iteration_range\u001b[39m=\u001b[39;49miteration_range,\n\u001b[0;32m   1167\u001b[0m             predict_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmargin\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39mif\u001b[39;49;00m output_margin \u001b[39melse\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39mvalue\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1168\u001b[0m             missing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmissing,\n\u001b[0;32m   1169\u001b[0m             base_margin\u001b[39m=\u001b[39;49mbase_margin,\n\u001b[0;32m   1170\u001b[0m             validate_features\u001b[39m=\u001b[39;49mvalidate_features,\n\u001b[0;32m   1171\u001b[0m         )\n\u001b[0;32m   1172\u001b[0m         \u001b[39mif\u001b[39;00m _is_cupy_array(predts):\n\u001b[0;32m   1173\u001b[0m             \u001b[39mimport\u001b[39;00m \u001b[39mcupy\u001b[39;00m  \u001b[39m# pylint: disable=import-error\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\core.py:2416\u001b[0m, in \u001b[0;36mBooster.inplace_predict\u001b[1;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[0;32m   2414\u001b[0m     data, fns, _ \u001b[39m=\u001b[39m _transform_pandas_df(data, enable_categorical)\n\u001b[0;32m   2415\u001b[0m     \u001b[39mif\u001b[39;00m validate_features:\n\u001b[1;32m-> 2416\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_features(fns)\n\u001b[0;32m   2417\u001b[0m \u001b[39mif\u001b[39;00m _is_list(data) \u001b[39mor\u001b[39;00m _is_tuple(data):\n\u001b[0;32m   2418\u001b[0m     data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(data)\n",
      "File \u001b[1;32mc:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\core.py:2968\u001b[0m, in \u001b[0;36mBooster._validate_features\u001b[1;34m(self, feature_names)\u001b[0m\n\u001b[0;32m   2962\u001b[0m \u001b[39mif\u001b[39;00m my_missing:\n\u001b[0;32m   2963\u001b[0m     msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m   2964\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mtraining data did not have the following fields: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2965\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m my_missing)\n\u001b[0;32m   2966\u001b[0m     )\n\u001b[1;32m-> 2968\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_names, feature_names))\n",
      "\u001b[1;31mValueError\u001b[0m: feature_names mismatch: ['index', 'absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', 'cloud_base_agl:m', 'dew_or_rime:idx', 'dew_point_2m:K', 'diffuse_rad:W', 'diffuse_rad_1h:J', 'direct_rad:W', 'direct_rad_1h:J', 'effective_cloud_cover:p', 'elevation:m', 'fresh_snow_12h:cm', 'fresh_snow_1h:cm', 'fresh_snow_24h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'is_day:idx', 'is_in_shadow:idx', 'msl_pressure:hPa', 'precip_5min:mm', 'precip_type_5min:idx', 'pressure_100m:hPa', 'pressure_50m:hPa', 'prob_rime:p', 'rain_water:kgm2', 'relative_humidity_1000hPa:p', 'sfc_pressure:hPa', 'snow_density:kgm3', 'snow_depth:cm', 'snow_drift:idx', 'snow_melt_10min:mm', 'snow_water:kgm2', 'sun_azimuth:d', 'sun_elevation:d', 'super_cooled_liquid_water:kgm2', 't_1000hPa:K', 'total_cloud_cover:p', 'visibility:m', 'wind_speed_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms', 'wind_speed_w_1000hPa:ms', 'hour', 'dayofmonth', 'dayofweek', 'quarter', 'month', 'year', 'dayofyear', 'location'] ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', 'cloud_base_agl:m', 'dew_or_rime:idx', 'dew_point_2m:K', 'diffuse_rad:W', 'diffuse_rad_1h:J', 'direct_rad:W', 'direct_rad_1h:J', 'effective_cloud_cover:p', 'elevation:m', 'fresh_snow_12h:cm', 'fresh_snow_1h:cm', 'fresh_snow_24h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'is_day:idx', 'is_in_shadow:idx', 'msl_pressure:hPa', 'precip_5min:mm', 'precip_type_5min:idx', 'pressure_100m:hPa', 'pressure_50m:hPa', 'prob_rime:p', 'rain_water:kgm2', 'relative_humidity_1000hPa:p', 'sfc_pressure:hPa', 'snow_density:kgm3', 'snow_depth:cm', 'snow_drift:idx', 'snow_melt_10min:mm', 'snow_water:kgm2', 'sun_azimuth:d', 'sun_elevation:d', 'super_cooled_liquid_water:kgm2', 't_1000hPa:K', 'total_cloud_cover:p', 'visibility:m', 'wind_speed_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms', 'wind_speed_w_1000hPa:ms', 'hour', 'dayofmonth', 'dayofweek', 'quarter', 'month', 'year', 'dayofyear', 'location']\nexpected index in input data"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBRegressor(\n",
    "    max_depth=7,\n",
    "    colsample_bytree=0.8,\n",
    "    eta=0.1,\n",
    "    n_estimators=90,\n",
    "    reg_alpha=0.01,\n",
    "    reg_lambda=0.01,\n",
    "    enable_categorical=True\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train[\"pv_measurement\"])\n",
    "\n",
    "# X_validate_a_loc = X_validate[X_validate_a[\"location\"]==\"A\"]\n",
    "# X_validate_a_loc = X_validate_a_loc.drop(\"location\", axis=1)\n",
    "\n",
    "prediction_a = model.predict(X_validate_a)\n",
    "prediction_b = model.predict(X_validate_b)\n",
    "prediction_c = model.predict(X_validate_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [23816, 10227]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\notebooks\\siverts_super_model.ipynb Cell 46\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/siver/NTNU%20H%C3%B8st%202023/Machine%20Learning/Group%20Project/tdt4173-forecasting/notebooks/siverts_super_model.ipynb#X61sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/siver/NTNU%20H%C3%B8st%202023/Machine%20Learning/Group%20Project/tdt4173-forecasting/notebooks/siverts_super_model.ipynb#X61sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mMAE: \u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/siver/NTNU%20H%C3%B8st%202023/Machine%20Learning/Group%20Project/tdt4173-forecasting/notebooks/siverts_super_model.ipynb#X61sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mround\u001b[39m(\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/siver/NTNU%20H%C3%B8st%202023/Machine%20Learning/Group%20Project/tdt4173-forecasting/notebooks/siverts_super_model.ipynb#X61sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         mean_absolute_error(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/siver/NTNU%20H%C3%B8st%202023/Machine%20Learning/Group%20Project/tdt4173-forecasting/notebooks/siverts_super_model.ipynb#X61sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m             y_true\u001b[39m=\u001b[39;49my_validate_a[\u001b[39m\"\u001b[39;49m\u001b[39mpv_measurement\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/siver/NTNU%20H%C3%B8st%202023/Machine%20Learning/Group%20Project/tdt4173-forecasting/notebooks/siverts_super_model.ipynb#X61sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m             y_pred\u001b[39m=\u001b[39;49mprediction_a,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/siver/NTNU%20H%C3%B8st%202023/Machine%20Learning/Group%20Project/tdt4173-forecasting/notebooks/siverts_super_model.ipynb#X61sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         ),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/siver/NTNU%20H%C3%B8st%202023/Machine%20Learning/Group%20Project/tdt4173-forecasting/notebooks/siverts_super_model.ipynb#X61sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39m3\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/siver/NTNU%20H%C3%B8st%202023/Machine%20Learning/Group%20Project/tdt4173-forecasting/notebooks/siverts_super_model.ipynb#X61sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     ),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/siver/NTNU%20H%C3%B8st%202023/Machine%20Learning/Group%20Project/tdt4173-forecasting/notebooks/siverts_super_model.ipynb#X61sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\sklearn\\metrics\\_regression.py:204\u001b[0m, in \u001b[0;36mmean_absolute_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39m@validate_params\u001b[39m(\n\u001b[0;32m    141\u001b[0m     {\n\u001b[0;32m    142\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39marray-like\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m     y_true, y_pred, \u001b[39m*\u001b[39m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, multioutput\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39muniform_average\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    151\u001b[0m ):\n\u001b[0;32m    152\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Mean absolute error regression loss.\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \n\u001b[0;32m    154\u001b[0m \u001b[39m    Read more in the :ref:`User Guide <mean_absolute_error>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39m    0.85...\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 204\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[39m=\u001b[39m _check_reg_targets(\n\u001b[0;32m    205\u001b[0m         y_true, y_pred, multioutput\n\u001b[0;32m    206\u001b[0m     )\n\u001b[0;32m    207\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    208\u001b[0m     output_errors \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39maverage(np\u001b[39m.\u001b[39mabs(y_pred \u001b[39m-\u001b[39m y_true), weights\u001b[39m=\u001b[39msample_weight, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\sklearn\\metrics\\_regression.py:99\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_reg_targets\u001b[39m(y_true, y_pred, multioutput, dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     66\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \n\u001b[0;32m     68\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[39m        correct keyword.\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m     check_consistent_length(y_true, y_pred)\n\u001b[0;32m    100\u001b[0m     y_true \u001b[39m=\u001b[39m check_array(y_true, ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    101\u001b[0m     y_pred \u001b[39m=\u001b[39m check_array(y_pred, ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    405\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    406\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 407\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    408\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    410\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [23816, 10227]"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"MAE: \",\n",
    "    round(\n",
    "        mean_absolute_error(\n",
    "            y_true=y_validate_a[\"pv_measurement\"],\n",
    "            y_pred=prediction_a,\n",
    "        ),\n",
    "        3,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  56.58\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"MAE: \",\n",
    "    round(\n",
    "        mean_absolute_error(\n",
    "            y_true=y_validate_b[\"pv_measurement\"],\n",
    "            y_pred=prediction_b,\n",
    "        ),\n",
    "        3,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  25.492\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"MAE: \",\n",
    "    round(\n",
    "        mean_absolute_error(\n",
    "            y_true=y_validate_c[\"pv_measurement\"],\n",
    "            y_pred=prediction_c,\n",
    "        ),\n",
    "        3,\n",
    "    ),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
