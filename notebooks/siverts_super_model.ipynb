{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2001,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan:\n",
    " - Load sets\n",
    " - Clean:\n",
    "    - remove duplicate entries from sets\n",
    "    - Drop the unimportant coloumns that we found\n",
    "    - Make querters into hours\n",
    "    - remove values from train_targets that are nan in both x and y sets\n",
    "    - make time features\n",
    "    - drop datetime coloumns\n",
    " - Set prep:\n",
    "    - 75/25 fordeling. 75% av OBSERVED er trening, resterende 25% er validation, 75% av ESTIMATED er trening, resterende 25% er validation\n",
    "- Concat the different locations into 1 set, where location is a feature as well\n",
    "   - 1 hot encoding\n",
    "- Make a time series model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2002,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A\n",
    "y_a = pd.read_parquet('../data/A/train_targets.parquet')\n",
    "X_test_estimated_a = pd.read_parquet('../data/A/X_test_estimated.parquet')\n",
    "X_train_estimated_a = pd.read_parquet('../data/A/X_train_estimated.parquet')\n",
    "X_train_observed_a = pd.read_parquet('../data/A/X_train_observed.parquet')\n",
    "\n",
    "# B\n",
    "y_b = pd.read_parquet('../data/B/train_targets.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('../data/B/X_test_estimated.parquet')\n",
    "X_train_estimated_b = pd.read_parquet('../data/B/X_train_estimated.parquet')\n",
    "X_train_observed_b = pd.read_parquet('../data/B/X_train_observed.parquet')\n",
    "\n",
    "# C\n",
    "y_c = pd.read_parquet('../data/C/train_targets.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('../data/C/X_test_estimated.parquet')\n",
    "X_train_estimated_c = pd.read_parquet('../data/C/X_train_estimated.parquet')\n",
    "X_train_observed_c = pd.read_parquet('../data/C/X_train_observed.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing duplicate entries from the sets if any exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2003,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that removes the duplicates if it finds duplicates in the specified coloumn\n",
    "def remove_duplicates_in_coloumn(df, col):\n",
    "    duplicate_mask = df[col].duplicated(keep=\"first\")\n",
    "    if duplicate_mask.any():\n",
    "        df = df[~duplicate_mask]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2004,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A\n",
    "y_a = remove_duplicates_in_coloumn(y_a, \"time\")\n",
    "X_test_estimated_a = remove_duplicates_in_coloumn(X_test_estimated_a, \"date_forecast\")\n",
    "X_train_estimated_a = remove_duplicates_in_coloumn(X_train_estimated_a, \"date_forecast\")\n",
    "X_train_observed_a = remove_duplicates_in_coloumn(X_train_observed_a, \"date_forecast\")\n",
    "\n",
    "#B\n",
    "y_b = remove_duplicates_in_coloumn(y_b, \"time\")\n",
    "X_test_estimated_b = remove_duplicates_in_coloumn(X_test_estimated_b, \"date_forecast\")\n",
    "X_train_estimated_b = remove_duplicates_in_coloumn(X_train_estimated_b, \"date_forecast\")\n",
    "X_train_observed_b = remove_duplicates_in_coloumn(X_train_observed_b, \"date_forecast\")\n",
    "\n",
    "#C\n",
    "y_c = remove_duplicates_in_coloumn(y_c, \"time\")\n",
    "X_test_estimated_c = remove_duplicates_in_coloumn(X_test_estimated_c, \"date_forecast\")\n",
    "X_train_estimated_c = remove_duplicates_in_coloumn(X_train_estimated_c, \"date_forecast\")\n",
    "X_train_observed_c = remove_duplicates_in_coloumn(X_train_observed_c, \"date_forecast\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2005,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of all estimated and observed sets\n",
    "\n",
    "list_of_all_estimated_and_observed_sets = [X_test_estimated_a, X_train_estimated_a, X_train_observed_a,\n",
    "                                           X_test_estimated_b, X_train_estimated_b, X_train_observed_b,\n",
    "                                           X_test_estimated_c, X_train_estimated_c, X_train_observed_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping some coloumns for the bants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2006,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for set in list_of_all_estimated_and_observed_sets:\n",
    "#     # set.drop(\"snow_density:kgm3\", axis=1, inplace=True)\n",
    "#     # these 2 had a lot of NaN values\n",
    "#     set.drop(\"ceiling_height_agl:m\", axis=1, inplace=True)\n",
    "#     set.drop(\"cloud_base_agl:m\", axis=1,inplace=True) # could potentially not drop this, but set all nan values to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting every 4 quarters into an whole hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2007,
   "metadata": {},
   "outputs": [],
   "source": [
    "## It might be better not to resample actually, but just use the actual hours that correspond perfectly\n",
    "\n",
    "def convert_df_into_hourly(df):\n",
    "    df.set_index(\"date_forecast\", inplace=True)\n",
    "    df = df.resample('1H').mean()\n",
    "    df.reset_index(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2008,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_estimated_a = convert_df_into_hourly(X_train_estimated_a)\n",
    "X_train_observed_a = convert_df_into_hourly(X_train_observed_a)\n",
    "X_test_estimated_a = convert_df_into_hourly(X_test_estimated_a)\n",
    "X_train_estimated_b = convert_df_into_hourly(X_train_estimated_b)\n",
    "X_train_observed_b = convert_df_into_hourly(X_train_observed_b)\n",
    "X_test_estimated_b = convert_df_into_hourly(X_test_estimated_b)\n",
    "X_train_estimated_c = convert_df_into_hourly(X_train_estimated_c)\n",
    "X_train_observed_c = convert_df_into_hourly(X_train_observed_c)\n",
    "X_test_estimated_c = convert_df_into_hourly(X_test_estimated_c)\n",
    "\n",
    "## We should probably do it for the test_sets here as well\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing date_calc from all estimated sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2009,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_estimated_a.drop(\"date_calc\", axis=1, inplace=True)\n",
    "X_train_estimated_a.drop(\"date_calc\", axis=1, inplace=True)\n",
    "X_test_estimated_b.drop(\"date_calc\", axis=1, inplace=True)\n",
    "X_train_estimated_b.drop(\"date_calc\", axis=1, inplace=True)\n",
    "X_test_estimated_c.drop(\"date_calc\", axis=1, inplace=True)\n",
    "X_train_estimated_c.drop(\"date_calc\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing all NaN from y_targets\n",
    " - Removing all NaN from y_targets and their corresponding dates in the X_train sets, both observed and estimated\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2010,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that returns train_targets, observed and estimated sets left after filtering away NaN\n",
    "def drop_nan_rows_in_target_and_train(y_df, observed_train_df, estimated_train_df):\n",
    "    y_df = y_df.dropna(subset=['pv_measurement'])\n",
    "    valid_dates = y_df['time']\n",
    "    observed_train_df = observed_train_df[observed_train_df['date_forecast'].isin(valid_dates)]\n",
    "    estimated_train_df = estimated_train_df[estimated_train_df['date_forecast'].isin(valid_dates)]\n",
    "    return (y_df, observed_train_df, estimated_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2011,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>pv_measurement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5913</th>\n",
       "      <td>2019-09-04 08:00:00</td>\n",
       "      <td>137.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5914</th>\n",
       "      <td>2019-09-04 09:00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5915</th>\n",
       "      <td>2019-09-04 10:00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5916</th>\n",
       "      <td>2019-09-04 11:00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5917</th>\n",
       "      <td>2019-09-04 12:00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    time  pv_measurement\n",
       "5913 2019-09-04 08:00:00           137.2\n",
       "5914 2019-09-04 09:00:00             0.0\n",
       "5915 2019-09-04 10:00:00             0.0\n",
       "5916 2019-09-04 11:00:00             0.0\n",
       "5917 2019-09-04 12:00:00             0.0"
      ]
     },
     "execution_count": 2011,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#doing the NaN filtering\n",
    "y_a, X_train_observed_a, X_train_estimated_a = drop_nan_rows_in_target_and_train(y_a, X_train_observed_a, X_train_estimated_a)\n",
    "y_b, X_train_observed_b, X_train_estimated_b = drop_nan_rows_in_target_and_train(y_b, X_train_observed_b, X_train_estimated_b)\n",
    "y_c, X_train_observed_c, X_train_estimated_c = drop_nan_rows_in_target_and_train(y_c, X_train_observed_c, X_train_estimated_c)\n",
    "\n",
    "y_c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2012,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_forecast</th>\n",
       "      <th>absolute_humidity_2m:gm3</th>\n",
       "      <th>air_density_2m:kgm3</th>\n",
       "      <th>ceiling_height_agl:m</th>\n",
       "      <th>clear_sky_energy_1h:J</th>\n",
       "      <th>clear_sky_rad:W</th>\n",
       "      <th>cloud_base_agl:m</th>\n",
       "      <th>dew_or_rime:idx</th>\n",
       "      <th>dew_point_2m:K</th>\n",
       "      <th>diffuse_rad:W</th>\n",
       "      <th>...</th>\n",
       "      <th>sun_azimuth:d</th>\n",
       "      <th>sun_elevation:d</th>\n",
       "      <th>super_cooled_liquid_water:kgm2</th>\n",
       "      <th>t_1000hPa:K</th>\n",
       "      <th>total_cloud_cover:p</th>\n",
       "      <th>visibility:m</th>\n",
       "      <th>wind_speed_10m:ms</th>\n",
       "      <th>wind_speed_u_10m:ms</th>\n",
       "      <th>wind_speed_v_10m:ms</th>\n",
       "      <th>wind_speed_w_1000hPa:ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5912</th>\n",
       "      <td>2019-09-04 08:00:00</td>\n",
       "      <td>6.625</td>\n",
       "      <td>1.22075</td>\n",
       "      <td>2287.250000</td>\n",
       "      <td>1320844.375</td>\n",
       "      <td>421.225006</td>\n",
       "      <td>2287.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>278.200012</td>\n",
       "      <td>67.974998</td>\n",
       "      <td>...</td>\n",
       "      <td>130.353500</td>\n",
       "      <td>25.754999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>283.725006</td>\n",
       "      <td>31.199999</td>\n",
       "      <td>49549.449219</td>\n",
       "      <td>1.250</td>\n",
       "      <td>-0.825</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5913</th>\n",
       "      <td>2019-09-04 09:00:00</td>\n",
       "      <td>6.275</td>\n",
       "      <td>1.21425</td>\n",
       "      <td>2679.074951</td>\n",
       "      <td>1681730.500</td>\n",
       "      <td>508.125000</td>\n",
       "      <td>2679.074951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>277.424988</td>\n",
       "      <td>76.625000</td>\n",
       "      <td>...</td>\n",
       "      <td>146.494507</td>\n",
       "      <td>30.204250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>284.799988</td>\n",
       "      <td>86.250000</td>\n",
       "      <td>52309.976562</td>\n",
       "      <td>1.500</td>\n",
       "      <td>-1.500</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5914</th>\n",
       "      <td>2019-09-04 10:00:00</td>\n",
       "      <td>5.900</td>\n",
       "      <td>1.20825</td>\n",
       "      <td>2983.750000</td>\n",
       "      <td>1936799.875</td>\n",
       "      <td>561.875000</td>\n",
       "      <td>2983.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>276.650024</td>\n",
       "      <td>112.574997</td>\n",
       "      <td>...</td>\n",
       "      <td>163.708252</td>\n",
       "      <td>33.027500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>285.899994</td>\n",
       "      <td>95.149994</td>\n",
       "      <td>53978.800781</td>\n",
       "      <td>2.000</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5915</th>\n",
       "      <td>2019-09-04 11:00:00</td>\n",
       "      <td>5.875</td>\n",
       "      <td>1.20350</td>\n",
       "      <td>3286.550049</td>\n",
       "      <td>2063526.250</td>\n",
       "      <td>578.025024</td>\n",
       "      <td>3286.550049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>276.649994</td>\n",
       "      <td>195.149994</td>\n",
       "      <td>...</td>\n",
       "      <td>181.566742</td>\n",
       "      <td>33.887001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>286.924988</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>54510.925781</td>\n",
       "      <td>1.950</td>\n",
       "      <td>-1.950</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5916</th>\n",
       "      <td>2019-09-04 12:00:00</td>\n",
       "      <td>6.150</td>\n",
       "      <td>1.20175</td>\n",
       "      <td>3453.425049</td>\n",
       "      <td>2051320.000</td>\n",
       "      <td>555.174988</td>\n",
       "      <td>3453.425049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>277.250000</td>\n",
       "      <td>243.375000</td>\n",
       "      <td>...</td>\n",
       "      <td>199.359009</td>\n",
       "      <td>32.668999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>287.500000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>54284.648438</td>\n",
       "      <td>1.525</td>\n",
       "      <td>-1.500</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date_forecast  absolute_humidity_2m:gm3  air_density_2m:kgm3  \\\n",
       "5912 2019-09-04 08:00:00                     6.625              1.22075   \n",
       "5913 2019-09-04 09:00:00                     6.275              1.21425   \n",
       "5914 2019-09-04 10:00:00                     5.900              1.20825   \n",
       "5915 2019-09-04 11:00:00                     5.875              1.20350   \n",
       "5916 2019-09-04 12:00:00                     6.150              1.20175   \n",
       "\n",
       "      ceiling_height_agl:m  clear_sky_energy_1h:J  clear_sky_rad:W  \\\n",
       "5912           2287.250000            1320844.375       421.225006   \n",
       "5913           2679.074951            1681730.500       508.125000   \n",
       "5914           2983.750000            1936799.875       561.875000   \n",
       "5915           3286.550049            2063526.250       578.025024   \n",
       "5916           3453.425049            2051320.000       555.174988   \n",
       "\n",
       "      cloud_base_agl:m  dew_or_rime:idx  dew_point_2m:K  diffuse_rad:W  ...  \\\n",
       "5912       2287.250000              0.0      278.200012      67.974998  ...   \n",
       "5913       2679.074951              0.0      277.424988      76.625000  ...   \n",
       "5914       2983.750000              0.0      276.650024     112.574997  ...   \n",
       "5915       3286.550049              0.0      276.649994     195.149994  ...   \n",
       "5916       3453.425049              0.0      277.250000     243.375000  ...   \n",
       "\n",
       "      sun_azimuth:d  sun_elevation:d  super_cooled_liquid_water:kgm2  \\\n",
       "5912     130.353500        25.754999                             0.0   \n",
       "5913     146.494507        30.204250                             0.0   \n",
       "5914     163.708252        33.027500                             0.0   \n",
       "5915     181.566742        33.887001                             0.0   \n",
       "5916     199.359009        32.668999                             0.0   \n",
       "\n",
       "      t_1000hPa:K  total_cloud_cover:p  visibility:m  wind_speed_10m:ms  \\\n",
       "5912   283.725006            31.199999  49549.449219              1.250   \n",
       "5913   284.799988            86.250000  52309.976562              1.500   \n",
       "5914   285.899994            95.149994  53978.800781              2.000   \n",
       "5915   286.924988           100.000000  54510.925781              1.950   \n",
       "5916   287.500000           100.000000  54284.648438              1.525   \n",
       "\n",
       "      wind_speed_u_10m:ms  wind_speed_v_10m:ms  wind_speed_w_1000hPa:ms  \n",
       "5912               -0.825                0.825                      0.0  \n",
       "5913               -1.500               -0.200                      0.0  \n",
       "5914               -2.000               -0.225                      0.0  \n",
       "5915               -1.950               -0.250                      0.0  \n",
       "5916               -1.500               -0.300                      0.0  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 2012,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_observed_c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2013,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that creates timefeatures based on the specified coloumn\n",
    "def create_time_features_based_on_coloun(df, col):   \n",
    "    df['hour'] = df[col].dt.hour\n",
    "    df['dayofmonth'] = df[col].dt.day\n",
    "    df['dayofweek'] = df[col].dt.dayofweek\n",
    "    df['quarter'] = df[col].dt.quarter\n",
    "    df['month'] = df[col].dt.month\n",
    "    df['year'] = df[col].dt.year\n",
    "    df['dayofyear'] = df[col].dt.dayofyear\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2014,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding time features\n",
    "\n",
    "#A\n",
    "X_train_estimated_a = create_time_features_based_on_coloun(X_train_estimated_a, \"date_forecast\")\n",
    "X_train_observed_a = create_time_features_based_on_coloun(X_train_observed_a, \"date_forecast\")\n",
    "X_test_estimated_a = create_time_features_based_on_coloun(X_test_estimated_a, \"date_forecast\")\n",
    "\n",
    "#B\n",
    "X_train_estimated_b = create_time_features_based_on_coloun(X_train_estimated_b, \"date_forecast\")\n",
    "X_train_observed_b = create_time_features_based_on_coloun(X_train_observed_b, \"date_forecast\")\n",
    "X_test_estimated_b = create_time_features_based_on_coloun(X_test_estimated_b, \"date_forecast\")\n",
    "\n",
    "#C\n",
    "X_train_estimated_c = create_time_features_based_on_coloun(X_train_estimated_c, \"date_forecast\")\n",
    "X_train_observed_c = create_time_features_based_on_coloun(X_train_observed_c, \"date_forecast\")\n",
    "X_test_estimated_c = create_time_features_based_on_coloun(X_test_estimated_c, \"date_forecast\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just showing the shapes to se if i havent f'ed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2015,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "(4418, 53)\n",
      "(29667, 53)\n",
      "34085\n",
      "(34085, 2)\n",
      "B\n",
      "(3625, 53)\n",
      "(29218, 53)\n",
      "32843\n",
      "(32844, 2)\n",
      "C\n",
      "(2954, 53)\n",
      "(23141, 53)\n",
      "26095\n",
      "(26095, 2)\n"
     ]
    }
   ],
   "source": [
    "# printing shapes\n",
    "print(\"A\")\n",
    "print(X_train_estimated_a.shape)\n",
    "print(X_train_observed_a.shape)\n",
    "print(X_train_estimated_a.shape[0]+X_train_observed_a.shape[0])\n",
    "print(y_a.shape)\n",
    "\n",
    "print(\"B\")\n",
    "print(X_train_estimated_b.shape)\n",
    "print(X_train_observed_b.shape)\n",
    "print(X_train_estimated_b.shape[0]+X_train_observed_b.shape[0])  ## this one seems to be of by 1??????\n",
    "print(y_b.shape)\n",
    "\n",
    "print(\"C\")\n",
    "print(X_train_estimated_c.shape)\n",
    "print(X_train_observed_c.shape)\n",
    "print(X_train_estimated_c.shape[0]+X_train_observed_c.shape[0])\n",
    "print(y_c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for B there is actually on row more in the target set for some reason, finding that row and deleting it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2016,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 time  pv_measurement\n",
      "0 2018-12-31 23:00:00             0.0\n"
     ]
    }
   ],
   "source": [
    "## Finding the row in y_b that does not have a match\n",
    "\n",
    "# renaming\n",
    "date_times_estimated = X_train_estimated_b['date_forecast']\n",
    "date_times_observed = X_train_observed_b['date_forecast']\n",
    "result_df = y_b[~y_b['time'].isin(date_times_estimated) & ~y_b['time'].isin(date_times_observed)]\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okey so let's remove that row and verify that everything is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2017,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n",
      "(3625, 53)\n",
      "(29218, 53)\n",
      "32843\n",
      "(32843, 2)\n"
     ]
    }
   ],
   "source": [
    "# removing row\n",
    "y_b = y_b[~y_b['time'].isin(result_df['time'])]\n",
    "\n",
    "# checking that the numbers add up again\n",
    "print(\"B\")\n",
    "print(X_train_estimated_b.shape)\n",
    "print(X_train_observed_b.shape)\n",
    "print(X_train_estimated_b.shape[0]+X_train_observed_b.shape[0])\n",
    "print(y_b.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing repeated indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dividing the sets into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2018,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_forecast</th>\n",
       "      <th>absolute_humidity_2m:gm3</th>\n",
       "      <th>air_density_2m:kgm3</th>\n",
       "      <th>ceiling_height_agl:m</th>\n",
       "      <th>clear_sky_energy_1h:J</th>\n",
       "      <th>clear_sky_rad:W</th>\n",
       "      <th>cloud_base_agl:m</th>\n",
       "      <th>dew_or_rime:idx</th>\n",
       "      <th>dew_point_2m:K</th>\n",
       "      <th>diffuse_rad:W</th>\n",
       "      <th>...</th>\n",
       "      <th>wind_speed_u_10m:ms</th>\n",
       "      <th>wind_speed_v_10m:ms</th>\n",
       "      <th>wind_speed_w_1000hPa:ms</th>\n",
       "      <th>hour</th>\n",
       "      <th>dayofmonth</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>dayofyear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-06-02 22:00:00</td>\n",
       "      <td>7.700</td>\n",
       "      <td>1.22825</td>\n",
       "      <td>1728.949951</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1728.949951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>280.299988</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.575</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2019</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-06-02 23:00:00</td>\n",
       "      <td>7.700</td>\n",
       "      <td>1.22350</td>\n",
       "      <td>1689.824951</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1689.824951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>280.299988</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.350</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2019</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-06-03 00:00:00</td>\n",
       "      <td>7.875</td>\n",
       "      <td>1.21975</td>\n",
       "      <td>1563.224976</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1563.224976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>280.649994</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.950</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2019</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-06-03 01:00:00</td>\n",
       "      <td>8.425</td>\n",
       "      <td>1.21800</td>\n",
       "      <td>1283.425049</td>\n",
       "      <td>208.649994</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1283.425049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>281.674988</td>\n",
       "      <td>0.300</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2019</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-06-03 02:00:00</td>\n",
       "      <td>8.950</td>\n",
       "      <td>1.21800</td>\n",
       "      <td>1003.500000</td>\n",
       "      <td>32468.150391</td>\n",
       "      <td>23.10</td>\n",
       "      <td>1003.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>282.500000</td>\n",
       "      <td>11.975</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.350</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2019</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date_forecast  absolute_humidity_2m:gm3  air_density_2m:kgm3  \\\n",
       "0 2019-06-02 22:00:00                     7.700              1.22825   \n",
       "1 2019-06-02 23:00:00                     7.700              1.22350   \n",
       "2 2019-06-03 00:00:00                     7.875              1.21975   \n",
       "3 2019-06-03 01:00:00                     8.425              1.21800   \n",
       "4 2019-06-03 02:00:00                     8.950              1.21800   \n",
       "\n",
       "   ceiling_height_agl:m  clear_sky_energy_1h:J  clear_sky_rad:W  \\\n",
       "0           1728.949951               0.000000             0.00   \n",
       "1           1689.824951               0.000000             0.00   \n",
       "2           1563.224976               0.000000             0.00   \n",
       "3           1283.425049             208.649994             0.75   \n",
       "4           1003.500000           32468.150391            23.10   \n",
       "\n",
       "   cloud_base_agl:m  dew_or_rime:idx  dew_point_2m:K  diffuse_rad:W  ...  \\\n",
       "0       1728.949951              0.0      280.299988          0.000  ...   \n",
       "1       1689.824951              0.0      280.299988          0.000  ...   \n",
       "2       1563.224976              0.0      280.649994          0.000  ...   \n",
       "3       1283.425049              0.0      281.674988          0.300  ...   \n",
       "4       1003.500000              0.0      282.500000         11.975  ...   \n",
       "\n",
       "   wind_speed_u_10m:ms  wind_speed_v_10m:ms  wind_speed_w_1000hPa:ms  hour  \\\n",
       "0               -3.575               -0.500                      0.0    22   \n",
       "1               -3.350                0.275                      0.0    23   \n",
       "2               -2.950                0.750                      0.0     0   \n",
       "3               -2.600                0.875                      0.0     1   \n",
       "4               -2.350                0.925                      0.0     2   \n",
       "\n",
       "   dayofmonth  dayofweek  quarter  month  year  dayofyear  \n",
       "0           2          6        2      6  2019        153  \n",
       "1           2          6        2      6  2019        153  \n",
       "2           3          0        2      6  2019        154  \n",
       "3           3          0        2      6  2019        154  \n",
       "4           3          0        2      6  2019        154  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 2018,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first renaming the date_forecast columns to time  (DOES NOT WORK)\n",
    "for df in list_of_all_estimated_and_observed_sets:               \n",
    "    df.rename(columns={'date_forecast': 'time'}, inplace=True)\n",
    "\n",
    "#maybe we can drop it here already actually\n",
    "X_train_observed_a.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data = first 75% of observed + first 75% of estimated\n",
    "Validation data = last 25% of observed + last 25% of estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2019,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30550, 53) (30550, 2)\n",
      "(29218, 53) (29218, 2)\n",
      "(19570, 53) (19570, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Making training and validation data for A\n",
    "percent_observed_train_a = 1\n",
    "percent_estimated_train_a = 0\n",
    "\n",
    "split_index_obs_a = int(len(X_train_observed_a)*percent_observed_train_a)\n",
    "X_train_observed_a_first_75 = X_train_observed_a[:split_index_obs_a]\n",
    "X_train_observed_a_last_25 = X_train_observed_a[split_index_obs_a:]\n",
    "\n",
    "split_index_est_a = int(len(X_train_estimated_a)*percent_estimated_train_a)\n",
    "X_train_estimated_a_first_75 = X_train_estimated_a[:split_index_est_a]\n",
    "X_train_estimated_a_last_25 = X_train_estimated_a[split_index_est_a:]\n",
    "\n",
    "X_train_a = pd.concat([X_train_observed_a_first_75, X_train_estimated_a_first_75])\n",
    "y_train_a = y_a[y_a[\"time\"].isin(X_train_a['date_forecast'])]\n",
    "print(X_train_a.shape, y_train_a.shape)\n",
    "\n",
    "X_validate_a = pd.concat([X_train_observed_a_last_25, X_train_estimated_a_last_25])\n",
    "y_validate_a = y_a[y_a[\"time\"].isin(X_validate_a['date_forecast'])]\n",
    "\n",
    "\n",
    "# making training and validation for B\n",
    "percent_observed_train_b = 1\n",
    "percent_estimated_train_b = 0\n",
    "\n",
    "split_index_obs_b = int(len(X_train_observed_b)*percent_observed_train_b)\n",
    "X_train_observed_b_first_75 = X_train_observed_b[:split_index_obs_b]\n",
    "X_train_observed_b_last_25 = X_train_observed_b[split_index_obs_b:]\n",
    "\n",
    "\n",
    "split_index_est_b = int(len(X_train_estimated_b)*percent_estimated_train_b)\n",
    "X_train_estimated_b_first_75 = X_train_estimated_b[:split_index_est_b]\n",
    "X_train_estimated_b_last_25 = X_train_estimated_b[split_index_est_b:]\n",
    "\n",
    "X_train_b = pd.concat([X_train_observed_b_first_75, X_train_estimated_b_first_75])\n",
    "y_train_b = y_b[y_b[\"time\"].isin(X_train_b['date_forecast'])]\n",
    "print(X_train_b.shape, y_train_b.shape)\n",
    "\n",
    "X_validate_b = pd.concat([X_train_observed_b_last_25, X_train_estimated_b_last_25])\n",
    "y_validate_b = y_b[y_b[\"time\"].isin(X_validate_b['date_forecast'])]\n",
    "\n",
    "# making training and validation for C\n",
    "percent_observed_train_c = 0.75\n",
    "percent_estimated_train_c = 0.75\n",
    "\n",
    "split_index_obs_c = int(len(X_train_observed_c)*percent_observed_train_c)\n",
    "X_train_observed_c_first_75 = X_train_observed_c[:split_index_obs_c]\n",
    "X_train_observed_c_last_25 = X_train_observed_c[split_index_obs_c:]\n",
    "\n",
    "split_index_est_c = int(len(X_train_estimated_c)*percent_estimated_train_c)\n",
    "X_train_estimated_c_first_75 = X_train_estimated_c[:split_index_est_c]\n",
    "X_train_estimated_c_last_25 = X_train_estimated_c[split_index_est_c:]\n",
    "\n",
    "X_train_c = pd.concat([X_train_observed_c_first_75, X_train_estimated_c_first_75])\n",
    "y_train_c = y_c[y_c[\"time\"].isin(X_train_c['date_forecast'])]\n",
    "print(X_train_c.shape, y_train_c.shape)\n",
    "\n",
    "X_validate_c = pd.concat([X_train_observed_c_last_25, X_train_estimated_c_last_25])\n",
    "y_validate_c = y_c[y_c[\"time\"].isin(X_validate_c['date_forecast'])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing repeated indexes here, think this might be the wrong place to do it, should probably to it for the concat BUT hEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2020,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_repeated_indexes(df, column_name, repeat_count=12):\n",
    "    \"\"\"\n",
    "    Find and return the indexes of rows with a specified number of repeated values in a given column.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame to search for repeated rows.\n",
    "    - column_name: Name of the column to check for repeated values.\n",
    "    - repeat_count: Number of repeated values required to consider a row as a match.\n",
    "\n",
    "    Returns:\n",
    "    - List of indexes for rows with the specified number of repeated values in the given column.\n",
    "    \"\"\"\n",
    "    df = df.reset_index()\n",
    "    repeated_indexes = []\n",
    "    temp_repeated_indexes = []\n",
    "    current_value = None\n",
    "    count = 0\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        value = row[column_name]\n",
    "\n",
    "        if value == current_value:\n",
    "            count += 1\n",
    "            temp_repeated_indexes.append(index)\n",
    "        else:\n",
    "            current_value = value\n",
    "            if count <= repeat_count:\n",
    "                temp_repeated_indexes = []\n",
    "                count = 1\n",
    "            else:\n",
    "                for i in temp_repeated_indexes:\n",
    "                    if i not in repeated_indexes:\n",
    "                        repeated_indexes.append(i)\n",
    "                temp_repeated_indexes = []\n",
    "                count = 1\n",
    "            \n",
    "\n",
    "    return repeated_indexes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2021,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing repeated indexes\n",
    "\n",
    "#A\n",
    "# print(X_train_a.shape, y_train_a.shape)\n",
    "# repeated_indices = find_repeated_indexes(y_train_a,\"pv_measurement\", 22)\n",
    "# y_train_a = y_train_a.reset_index()\n",
    "# y_train_a = y_train_a.drop(repeated_indices)\n",
    "# X_train_a = X_train_a[X_train_a[\"date_forecast\"].isin(y_train_a[\"time\"])]\n",
    "# print(X_train_a.shape, y_train_a.shape)\n",
    "\n",
    "# repeated_indices = find_repeated_indexes(y_validate_a,\"pv_measurement\",22)\n",
    "# y_validate_a = y_validate_a.reset_index()\n",
    "# y_validate_a = y_validate_a.drop(repeated_indices)\n",
    "# X_validate_a = X_validate_a[X_validate_a[\"date_forecast\"].isin(y_validate_a[\"time\"])]\n",
    "\n",
    "# y_train_a.reset_index(drop=True, inplace=True)\n",
    "# X_train_a.reset_index(drop=True, inplace=True)\n",
    "# y_validate_a.reset_index(drop=True, inplace=True)\n",
    "# X_validate_a.reset_index(drop=True, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2022,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29218, 53) (29218, 2)\n",
      "(22193, 53) (22193, 3)\n"
     ]
    }
   ],
   "source": [
    "#B  -  works reeaaaly well for B\n",
    "print(X_train_b.shape, y_train_b.shape)\n",
    "repeated_indices = find_repeated_indexes(y_train_b,\"pv_measurement\", 22)\n",
    "y_train_b = y_train_b.reset_index()\n",
    "y_train_b = y_train_b.drop(repeated_indices)\n",
    "X_train_b = X_train_b[X_train_b[\"date_forecast\"].isin(y_train_b[\"time\"])]\n",
    "print(X_train_b.shape, y_train_b.shape)\n",
    "\n",
    "repeated_indices = find_repeated_indexes(y_validate_b,\"pv_measurement\",22)\n",
    "y_validate_b = y_validate_b.reset_index()\n",
    "y_validate_b = y_validate_b.drop(repeated_indices)\n",
    "X_validate_b = X_validate_b[X_validate_b[\"date_forecast\"].isin(y_validate_b[\"time\"])]\n",
    "\n",
    "y_train_b.reset_index(drop=True, inplace=True)\n",
    "X_train_b.reset_index(drop=True, inplace=True)\n",
    "y_validate_b.reset_index(drop=True, inplace=True)\n",
    "X_validate_b.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2023,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C\n",
    "# print(X_train_c.shape, y_train_c.shape)\n",
    "# repeated_indices = find_repeated_indexes(y_train_c,\"pv_measurement\", 24)\n",
    "# y_train_c = y_train_c.reset_index()\n",
    "# y_train_c = y_train_c.drop(repeated_indices)\n",
    "# X_train_c = X_train_c[X_train_c[\"date_forecast\"].isin(y_train_c[\"time\"])]\n",
    "# print(X_train_c.shape, y_train_c.shape)\n",
    "\n",
    "# repeated_indices = find_repeated_indexes(y_validate_c,\"pv_measurement\",24)\n",
    "# y_validate_c = y_validate_c.reset_index()\n",
    "# y_validate_c = y_validate_c.drop(repeated_indices)\n",
    "# X_validate_c = X_validate_c[X_validate_c[\"date_forecast\"].isin(y_validate_c[\"time\"])]\n",
    "\n",
    "# y_train_c.reset_index(drop=True, inplace=True)\n",
    "# X_train_c.reset_index(drop=True, inplace=True)\n",
    "# y_validate_c.reset_index(drop=True, inplace=True)\n",
    "# X_validate_c.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing time feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2024,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siver\\AppData\\Local\\Temp\\ipykernel_26408\\3483724575.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_train_a.drop(\"time\", axis=1, inplace=True)\n",
      "C:\\Users\\siver\\AppData\\Local\\Temp\\ipykernel_26408\\3483724575.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_validate_a.drop(\"time\", axis=1, inplace=True)\n",
      "C:\\Users\\siver\\AppData\\Local\\Temp\\ipykernel_26408\\3483724575.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_train_c.drop(\"time\", axis=1, inplace=True)\n",
      "C:\\Users\\siver\\AppData\\Local\\Temp\\ipykernel_26408\\3483724575.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_validate_c.drop(\"time\", axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "X_train_a.drop(\"date_forecast\", axis=1, inplace=True)\n",
    "y_train_a.drop(\"time\", axis=1, inplace=True)\n",
    "X_validate_a.drop(\"date_forecast\", axis=1, inplace=True)\n",
    "y_validate_a.drop(\"time\", axis=1, inplace=True)\n",
    "\n",
    "X_train_b.drop(\"date_forecast\", axis=1, inplace=True)\n",
    "y_train_b.drop(\"time\", axis=1, inplace=True)\n",
    "X_validate_b.drop(\"date_forecast\", axis=1, inplace=True)\n",
    "y_validate_b.drop(\"time\", axis=1, inplace=True)\n",
    "\n",
    "X_train_c.drop(\"date_forecast\", axis=1, inplace=True)\n",
    "y_train_c.drop(\"time\", axis=1, inplace=True)\n",
    "X_validate_c.drop(\"date_forecast\", axis=1, inplace=True)\n",
    "y_validate_c.drop(\"time\", axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding location features to the sets before merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2025,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siver\\AppData\\Local\\Temp\\ipykernel_26408\\816523439.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_train_a[\"location\"] = 1\n",
      "C:\\Users\\siver\\AppData\\Local\\Temp\\ipykernel_26408\\816523439.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_validate_a[\"location\"] = 1\n",
      "C:\\Users\\siver\\AppData\\Local\\Temp\\ipykernel_26408\\816523439.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_train_c[\"location\"] = 3\n",
      "C:\\Users\\siver\\AppData\\Local\\Temp\\ipykernel_26408\\816523439.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_validate_c[\"location\"] = 3\n"
     ]
    }
   ],
   "source": [
    "X_train_a[\"location\"] =  1 \n",
    "y_train_a[\"location\"] = 1\n",
    "X_validate_a[\"location\"] = 1\n",
    "y_validate_a[\"location\"] = 1\n",
    "\n",
    "X_train_b[\"location\"] = 2\n",
    "y_train_b[\"location\"] = 2\n",
    "X_validate_b[\"location\"] = 2\n",
    "y_validate_b[\"location\"] = 2\n",
    "\n",
    "X_train_c[\"location\"] = 3\n",
    "y_train_c[\"location\"] = 3\n",
    "X_validate_c[\"location\"] = 3\n",
    "y_validate_c[\"location\"] = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging tranining data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2026,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72313, 53)\n",
      "(72313, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.concat([X_train_a, X_train_b, X_train_c])\n",
    "y_train = pd.concat([y_train_a, y_train_b, y_train_c])\n",
    "\n",
    "X_validate = pd.concat([X_validate_a, X_validate_b, X_validate_c])\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2027,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding and dropping location feature   some fuckery here\n",
    "# one_hot = pd.get_dummies(X_train[\"location\"]).astype(int)\n",
    "# X_train = X_train.drop(\"location\", axis=1)\n",
    "# X_train = pd.merge(X_train, one_hot, left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "# one_hot = pd.get_dummies(X_validate[\"location\"]).astype(int)\n",
    "# X_validate = pd.merge(X_validate, one_hot, left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test an XG boost only on A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2028,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "c:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n",
      "c:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype):\n",
      "c:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "c:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "c:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n",
      "c:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype):\n",
      "c:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "c:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "c:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n",
      "c:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype):\n",
      "c:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "c:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "c:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n",
      "c:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype):\n",
      "c:\\Users\\siver\\NTNU Høst 2023\\Machine Learning\\Group Project\\tdt4173-forecasting\\.venv\\lib\\site-packages\\xgboost\\data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBRegressor(\n",
    "    max_depth=7,\n",
    "    colsample_bytree=0.8,\n",
    "    eta=0.1,\n",
    "    n_estimators=90,\n",
    "    reg_alpha=0.01,\n",
    "    reg_lambda=0.01,\n",
    "    enable_categorical=True\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train[\"pv_measurement\"])\n",
    "\n",
    "# X_validate_a_loc = X_validate[X_validate_a[\"location\"]==\"A\"]\n",
    "# X_validate_a_loc = X_validate_a_loc.drop(\"location\", axis=1)\n",
    "\n",
    "prediction_a = model.predict(X_validate_a)\n",
    "prediction_b = model.predict(X_validate_b)\n",
    "prediction_c = model.predict(X_validate_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2029,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  124.091\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"MAE: \",\n",
    "    round(\n",
    "        mean_absolute_error(\n",
    "            y_true=y_validate_a[\"pv_measurement\"],\n",
    "            y_pred=prediction_a,\n",
    "        ),\n",
    "        3,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2030,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  20.437\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"MAE: \",\n",
    "    round(\n",
    "        mean_absolute_error(\n",
    "            y_true=y_validate_b[\"pv_measurement\"],\n",
    "            y_pred=prediction_b,\n",
    "        ),\n",
    "        3,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2031,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  19.701\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"MAE: \",\n",
    "    round(\n",
    "        mean_absolute_error(\n",
    "            y_true=y_validate_c[\"pv_measurement\"],\n",
    "            y_pred=prediction_c,\n",
    "        ),\n",
    "        3,\n",
    "    ),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
