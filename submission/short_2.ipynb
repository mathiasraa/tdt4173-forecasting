{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div\n",
    "  style=\"\n",
    "    display: flex;\n",
    "    justify-content: center;\n",
    "    flex-direction: column;\n",
    "    align-items: center;\n",
    "  \"\n",
    ">\n",
    "<div style=\"width: 400px; padding: 20px; background: #111; border: 1px solid #333; color: #eee\">\n",
    "  <p style=\"opacity: 0.5; margin-bottom: -10px\"><i>TDT4173 Machine Learning 2023</i></p>\n",
    "  <h1 style=\"margin-bottom: -5px\"><b>Short Notebook 2</b></h1>\n",
    "  <h3>Kaggle Name: NeuralNet Ninjas</h3>\n",
    "  <br />\n",
    "  <h3>Team Members:</h3>\n",
    "  <table style=\"margin: 0 auto; width: 100%; text-align: left\">\n",
    "    <tr style=\"background: #222\">\n",
    "      <th style=\"border-width: 0.5px; border-color: #555\">Name</th>\n",
    "      <th style=\"border-width: 0.5px; border-color: #555\">Student ID</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"border-width: 0.5px; border-color: #555\">Antonsen Eggen, Sivert</td>\n",
    "      <td style=\"border-width: 0.5px; border-color: #555\">123 45 678</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"border-width: 0.5px; border-color: #555\">Broch Grude, Kristoffer VI Nicolay</td>\n",
    "      <td style=\"border-width: 0.5px; border-color: #555\">123 45 678</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"border-width: 0.5px; border-color: #555\">Raa, Mathias</td>\n",
    "      <td style=\"border-width: 0.5px; border-color: #555\">123 45 678</td>\n",
    "    </tr>\n",
    "  </table>\n",
    "  <br />\n",
    "  <h3 style=\"margin-bottom: -5px\">Runtime: x hours</h3>\n",
    "  <p style=\"opacity: 0.5;\"><i>Specs: MacBook Pro 2021, Apple M1 Pro Chip, 16 GB RAM</i></p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of all model predictions for final ensambling\n",
    "test_result = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 – Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 – Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Stack**\n",
    "1. Target Encoded Catboost\n",
    "2. Simple LightGBM\n",
    "3. Target Log Transform Catboost\n",
    "4. Location Specific Catboost\n",
    "5. Location Specific AutoGluon\n",
    "6. Location Specific FastAI Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 – Target Encoded Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_general_catboost_transform = data_catboost.copy()\n",
    "\n",
    "drop_cols = [\n",
    "    \"cloud_base_agl:m\",\n",
    "    \"ceiling_height_agl:m\",\n",
    "    \"snow_density:kgm3\",\n",
    "    \"hour\",\n",
    "    \"month\",\n",
    "    \"date_calc\",\n",
    "    \"date_forecast\",\n",
    "    \"clear_sky_rad:W\",\n",
    "]\n",
    "\n",
    "data_general_catboost_transform[\"pv_measurement\"] = data_general_catboost_transform[\"pv_measurement\"]\n",
    "\n",
    "mean_target = (\n",
    "    data_general_catboost_transform[\n",
    "        data_general_catboost_transform.data_type.isin([\"observed\", \"estimated\"])\n",
    "    ]\n",
    "    .groupby([\"location\", \"month\", \"hour\"])[\"pv_measurement\"]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "location_mapping = mean_target.to_dict()\n",
    "\n",
    "data_general_catboost_transform[\n",
    "    \"location_encoded\"\n",
    "] = data_general_catboost_transform.apply(\n",
    "    lambda x: location_mapping.get((x[\"location\"], x[\"month\"], x[\"hour\"]), None), axis=1\n",
    ")\n",
    "\n",
    "\n",
    "data_general_catboost_transform = data_general_catboost_transform.drop(\n",
    "    drop_cols, axis=1\n",
    ")\n",
    "\n",
    "ignore_cols = [\n",
    "    \"location\",\n",
    "    \"dew_or_rime:idx\",\n",
    "    \"is_day:idx\",\n",
    "    \"is_in_shadow:idx\",\n",
    "    \"pv_measurement\",\n",
    "    \"data_type\",\n",
    "]\n",
    "\n",
    "cols = [\n",
    "    col for col in data_general_catboost_transform.columns if col not in ignore_cols\n",
    "]\n",
    "\n",
    "data_general_catboost_transform, _, y_scaler = scale_data(\n",
    "    data_general_catboost_transform, cols=cols\n",
    ")\n",
    "\n",
    "# Create training and test data\n",
    "train = (\n",
    "    data_general_catboost_transform[\n",
    "        data_general_catboost_transform[\"data_type\"].isin([\"observed\", \"estimated\"])\n",
    "    ]\n",
    "    .drop(columns=[\"data_type\"])\n",
    "    .copy()\n",
    ")\n",
    "test = (\n",
    "    data_general_catboost_transform[\n",
    "        data_general_catboost_transform[\"data_type\"] == \"test\"\n",
    "    ]\n",
    "    .drop(columns=[\"data_type\"])\n",
    "    .copy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_measurement = train[\"pv_measurement\"]\n",
    "train.drop(\"pv_measurement\", axis=1, inplace=True)\n",
    "\n",
    "cat_features = [0]\n",
    "model_params = {\n",
    "    \"iterations\": 1000,\n",
    "    \"depth\": 9,\n",
    "    \"loss_function\": \"MAE\",\n",
    "    \"cat_features\": cat_features,\n",
    "}\n",
    "\n",
    "# Initialize lists to store the predictions\n",
    "all_preds = pd.DataFrame()\n",
    "temp_test_result = []\n",
    "\n",
    "# Create a CatBoostRegressor instance for this fold\n",
    "model = catboost.CatBoostRegressor(**model_params)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(train, pv_measurement)\n",
    "\n",
    "# Get predictions for the validation set\n",
    "preds = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result[\"encoded_catboost\"] = pd.DataFrame(\n",
    "    y_scaler.inverse_transform(preds.reshape(-1, 1)), columns=[\"pv_measurement\"]\n",
    ")\n",
    "test_result[\"encoded_catboost\"][\"pv_measurement\"] = test_result[\n",
    "    \"encoded_catboost\"\n",
    "].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 – Simple LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_general_lgb = data_catboost.copy()\n",
    "\n",
    "drop_cols = [\n",
    "    \"cloud_base_agl:m\",\n",
    "    \"ceiling_height_agl:m\",\n",
    "    \"snow_density:kgm3\",\n",
    "    \"month_sin\",\n",
    "    \"hour_sin\",\n",
    "    \"month_cos\",\n",
    "    \"hour_cos\",\n",
    "    \"date_calc\",\n",
    "    \"sun_azimuth:d_sin\",\n",
    "    \"sun_azimuth:d_cos\",\n",
    "    \"date_forecast\",\n",
    "    \"elevation:m\",\n",
    "    \"precip_5min:mm\",\n",
    "    \"precip_type_5min:idx\",\n",
    "    \"snow_drift:idx\",\n",
    "    \"wind_speed_u_10m:ms\",\n",
    "    \"wind_speed_v_10m:ms\",\n",
    "    \"wind_speed_w_1000hPa:ms\",\n",
    "]\n",
    "\n",
    "data_general_lgb = data_general_lgb.drop(drop_cols, axis=1)\n",
    "data_general_lgb.columns = [\n",
    "    column.replace(\":\", \"_\") for column in data_general_lgb.columns\n",
    "]\n",
    "\n",
    "ignore_cols = [\n",
    "    \"location\",\n",
    "    \"is_day_idx\",\n",
    "    \"is_in_shadow_idx\",\n",
    "    \"pv_measurement\",\n",
    "    \"data_type\",\n",
    "]\n",
    "\n",
    "data_general_lgb[\"is_day_idx\"] = data_general_lgb[\"is_day_idx\"].astype(\"int\")\n",
    "data_general_lgb[\"is_in_shadow_idx\"] = data_general_lgb[\"is_in_shadow_idx\"].astype(\"int\")\n",
    "data_general_lgb[\"dew_or_rime_idx\"] = data_general_lgb[\"dew_or_rime_idx\"].astype(\"int\")\n",
    "\n",
    "cols = [col for col in data_general_lgb.columns if col not in ignore_cols]\n",
    "\n",
    "data_general_lgb, _, y_scaler = scale_data(data_general_lgb, cols=cols)\n",
    "\n",
    "# location one-hot\n",
    "data_general_lgb = pd.get_dummies(data_general_lgb, columns=[\"location\"])\n",
    "\n",
    "# Create training and test data\n",
    "test = data_general_lgb[data_general_lgb[\"data_type\"].isin([\"test\"])].drop(columns=[\"data_type\"]).copy()\n",
    "train = data_general_lgb[data_general_lgb[\"data_type\"].isin([\"observed\", \"estimated\"])].drop(columns=[\"data_type\"]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_measurement = train[\"pv_measurement\"]\n",
    "\n",
    "model_params = {\n",
    "    \"metric\": \"MSE\",\n",
    "    \"max_depth\": 6,\n",
    "    \"num_rounds\": 200,\n",
    "    \"colsample_bytree\": 0.6,\n",
    "    \"learning_rate\": 0.04,\n",
    "    \"num_leaves\": 140,\n",
    "}\n",
    "\n",
    "# Create a CatBoostRegressor instance for this fold\n",
    "model = lgb.LGBMRegressor(**model_params)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(train.drop(columns=[\"pv_measurement\"]), pv_measurement)\n",
    "\n",
    "# Get predictions for the validation set\n",
    "preds = model.predict(test.drop(columns=[\"pv_measurement\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result[\"general_lightgbm\"] = pd.DataFrame(y_scaler.inverse_transform(preds.reshape(-1, 1)), columns=[\"pv_measurement\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 – Target Log Transform Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_general_catboost_transform = data_catboost.copy()\n",
    "\n",
    "\n",
    "# Drop columns that are not needed\n",
    "drop_cols = [\n",
    "    \"cloud_base_agl:m\",\n",
    "    \"ceiling_height_agl:m\",\n",
    "    \"snow_density:kgm3\",\n",
    "    \"hour\",\n",
    "    \"month\",\n",
    "    \"date_calc\",\n",
    "    \"date_forecast\",\n",
    "    \"clear_sky_rad:W\",\n",
    "    # \"sun_azimuth:d\",\n",
    "]\n",
    "\n",
    "data_general_catboost_transform[\"pv_measurement\"] = np.log1p(\n",
    "    data_general_catboost_transform[\"pv_measurement\"]\n",
    ")\n",
    "\n",
    "data_general_catboost_transform = data_general_catboost_transform.drop(\n",
    "    drop_cols, axis=1\n",
    ")\n",
    "\n",
    "ignore_cols = [\n",
    "    \"location\",\n",
    "    \"dew_or_rime:idx\",\n",
    "    \"is_day:idx\",\n",
    "    \"is_in_shadow:idx\",\n",
    "    \"pv_measurement\",\n",
    "    \"data_type\",\n",
    "]\n",
    "\n",
    "cols = [\n",
    "    col for col in data_general_catboost_transform.columns if col not in ignore_cols\n",
    "]\n",
    "\n",
    "data_general_catboost_transform, _, y_scaler = scale_data(\n",
    "    data_general_catboost_transform, cols=cols\n",
    ")\n",
    "\n",
    "# Create training and test data\n",
    "train = (\n",
    "    data_general_catboost_transform[\n",
    "        data_general_catboost_transform[\"data_type\"].isin([\"observed\", \"estimated\"])\n",
    "    ]\n",
    "    .drop(columns=[\"data_type\"])\n",
    "    .copy()\n",
    ")\n",
    "test = (\n",
    "    data_general_catboost_transform[\n",
    "        data_general_catboost_transform[\"data_type\"] == \"test\"\n",
    "    ]\n",
    "    .drop(columns=[\"data_type\"])\n",
    "    .copy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_measurement = train[\"pv_measurement\"]\n",
    "train.drop(\"pv_measurement\", axis=1, inplace=True)\n",
    "\n",
    "cat_features = [0]\n",
    "model_params = {\n",
    "    \"iterations\": 1000,\n",
    "    \"depth\": 9,\n",
    "    \"loss_function\": \"MAE\",\n",
    "    \"cat_features\": cat_features,\n",
    "}\n",
    "\n",
    "# For final predictions\n",
    "# Create a CatBoostRegressor instance for this fold\n",
    "model = catboost.CatBoostRegressor(**model_params)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(train, pv_measurement)\n",
    "\n",
    "# Get predictions for the validation set\n",
    "preds = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result[\"transform_catboost\"] = pd.DataFrame(\n",
    "    y_scaler.inverse_transform(preds.reshape(-1, 1)), columns=[\"pv_measurement\"]\n",
    ")\n",
    "test_result[\"transform_catboost\"][\"pv_measurement\"] = np.expm1(\n",
    "    test_result[\"transform_catboost\"].values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4 – Location Specific Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_location = data_catboost.copy()\n",
    "\n",
    "# Drop columns that are not needed\n",
    "drop_cols = [\n",
    "    \"cloud_base_agl:m\",\n",
    "    \"ceiling_height_agl:m\",\n",
    "    \"snow_density:kgm3\",\n",
    "    \"hour\",\n",
    "    \"month\",\n",
    "    \"date_calc\",\n",
    "    \"date_forecast\",\n",
    "    # \"sun_azimuth:d\",\n",
    "]\n",
    "\n",
    "catboost_location = catboost_location.drop(drop_cols, axis=1)\n",
    "\n",
    "# transform the target variable by subtracting direct radiation and diffuse radiation:\n",
    "catboost_location[\"estimated\"] = np.where(\n",
    "    catboost_location.data_type.isin([\"estimated\", \"test\"]), 1, 0\n",
    ")\n",
    "\n",
    "# scaling the data\n",
    "ignore_cols = [\n",
    "    \"location\",\n",
    "    \"dew_or_rime:idx\",\n",
    "    \"is_day:idx\",\n",
    "    \"is_in_shadow:idx\",\n",
    "    \"data_type\",\n",
    "    \"estimated\"\n",
    "]\n",
    "\n",
    "cols = [col for col in catboost_location.columns if col not in ignore_cols]\n",
    "\n",
    "# Create training and test data\n",
    "\n",
    "train_all = catboost_location[catboost_location.data_type.isin([\"observed\", \"estimated\"])].drop(columns=\"data_type\").copy()\n",
    "test = catboost_location[catboost_location.data_type == \"test\"].drop(columns=\"data_type\").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"iterations\": 1000,\n",
    "    \"depth\": 9,\n",
    "    \"loss_function\": \"MAE\",\n",
    "}\n",
    "\n",
    "final_preds = []\n",
    "\n",
    "for location in (\"A\", \"B\", \"C\"):\n",
    "    train = train_all[train_all.location == location].drop(columns=[\"location\"])\n",
    "\n",
    "    train_pool = catboost.Pool(data=train.drop(columns=\"pv_measurement\"), label=train[\"pv_measurement\"], cat_features=[\"estimated\"])\n",
    "    test_pool = catboost.Pool(data=test[test.location == location].drop(columns=[\"pv_measurement\", \"location\"]), cat_features=[\"estimated\"])\n",
    "\n",
    "    # Create a CatBoostRegressor instance for this fold\n",
    "    model = catboost.CatBoostRegressor(**model_params)\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(train_pool)\n",
    "\n",
    "    # Get predictions for the validation set\n",
    "    preds = model.predict(test_pool)\n",
    "\n",
    "    # Invert scaling for the validation set predictions\n",
    "    prediction_df = pd.DataFrame(preds, columns=[\"prediction\"], index=test[test.location == location].index)\n",
    "    prediction_df[\"location\"] = location\n",
    "\n",
    "    # Append the predictions to the list\n",
    "    final_preds.append(prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result[\"location_catboost\"] = pd.concat(final_preds).reset_index().sort_values([\"location\", \"index\"]).drop(columns=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5 – Location Specific AutoGluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_autogluon = data_catboost.copy()\n",
    "\n",
    "drop_cols = [\n",
    "    \"ceiling_height_agl:m\",\n",
    "    \"cloud_base_agl:m\",\n",
    "    \"snow_density:kgm3\",\n",
    "    \"elevation:m\",\n",
    "    \"snow_drift:idx\",\n",
    "    \"date_calc\",\n",
    "    \"sun_azimuth:d_sin\",\n",
    "    \"sun_azimuth:d_cos\",\n",
    "    \"prob_rime:p\",\n",
    "    \"fresh_snow_1h:cm\",\n",
    "    \"dew_or_rime:idx\",\n",
    "    \"fresh_snow_3h:cm\",\n",
    "    \"hour\",\n",
    "    \"month\",\n",
    "    \"hour_sin\",\n",
    "    \"hour_cos\",\n",
    "    \"month_sin\",\n",
    "    \"month_cos\",\n",
    "    \"year\",\n",
    "    \"day\",\n",
    "    \"global_rad\"\n",
    "]\n",
    "\n",
    "data_autogluon[\"year\"] = data_autogluon[\"date_forecast\"].dt.year\n",
    "data_autogluon[\"day\"] = data_autogluon[\"date_forecast\"].dt.dayofyear\n",
    "data_autogluon[\"global_rad\"] = data_autogluon[\"direct_rad:W\"] + data_autogluon[\"diffuse_rad:W\"]\n",
    "\n",
    "\n",
    "data_autogluon[\"global_rad_-1\"] = (\n",
    "    data_autogluon.groupby([\"location\", \"year\", \"day\"])[\"global_rad\"].shift(-1).fillna(0)\n",
    ")\n",
    "\n",
    "data_autogluon[\"precip_5min:mm-2\"] = (\n",
    "    data_autogluon.groupby([\"location\", \"year\", \"day\"])[\"precip_5min:mm\"]\n",
    "    .shift(-2)\n",
    "    .fillna(0)\n",
    ")\n",
    "data_autogluon[\"rain_water:kgm2-2\"] = (\n",
    "    data_autogluon.groupby([\"location\", \"year\", \"day\"])[\"rain_water:kgm2\"]\n",
    "    .shift(-2)\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "# avg rainfall per month calculated from estimated/observed\n",
    "sum_rainfall = (\n",
    "    data_autogluon.loc[data_autogluon[\"data_type\"].isin([\"observed\", \"estimated\"])]\n",
    "    .groupby(\"month\")[\"precip_5min:mm\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"precip_5min:mm\": \"sum_rainfall\"})\n",
    ")\n",
    "\n",
    "data_autogluon = data_autogluon.merge(sum_rainfall, on=\"month\", how=\"left\")\n",
    "\n",
    "# avg tmp per month calculated from estimated/observed\n",
    "sum_wind = (\n",
    "    data_autogluon.loc[data_autogluon[\"data_type\"].isin([\"observed\", \"estimated\"])]\n",
    "    .groupby(\"month\")[\"wind_speed_10m:ms\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"wind_speed_10m:ms\": \"sum_wind\"})\n",
    ")\n",
    "\n",
    "\n",
    "data_autogluon = data_autogluon.merge(sum_wind, on=\"month\", how=\"left\")\n",
    "\n",
    "median_wind_hourly = (\n",
    "    data_autogluon.loc[data_autogluon[\"data_type\"].isin([\"observed\", \"estimated\"])]\n",
    "    .groupby([\"month\", \"hour\"])[\"wind_speed_10m:ms\"]\n",
    "    .median()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"wind_speed_10m:ms\": \"median_wind_hourly\"})\n",
    ")\n",
    "\n",
    "data_autogluon = data_autogluon.merge(median_wind_hourly, on=[\"month\", \"hour\"], how=\"left\")\n",
    "\n",
    "\n",
    "data_autogluon[\"estimated\"] = np.where(\n",
    "    data_autogluon[\"data_type\"].isin([\"estimated\", \"test\"]), \"e\", \"o\"\n",
    ")\n",
    "\n",
    "data_autogluon = data_autogluon.drop(drop_cols, axis=1)\n",
    "\n",
    "test = data_autogluon[data_autogluon[\"data_type\"].isin([\"test\"])].copy().drop(columns=[\"data_type\"])\n",
    "\n",
    "train, val = train_test_split(\n",
    "    data_autogluon[data_autogluon[\"data_type\"].isin([\"estimated\"])],\n",
    "    test_size=0.40,\n",
    "    random_state=1111,\n",
    ")\n",
    "\n",
    "train = pd.concat([data_autogluon[data_autogluon[\"data_type\"].isin([\"observed\"])], train])\n",
    "train = train.drop(columns=[\"data_type\"])\n",
    "val = val.drop(columns=[\"data_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train AutoGluon model\n",
    "tabular_predictor = TabularPredictor(label=\"pv_measurement\", eval_metric=\"mae\")\n",
    "\n",
    "predictor_A = tabular_predictor.fit(\n",
    "    train_data=train[train.location == \"A\"],\n",
    "    tuning_data=val[val.location == \"A\"],\n",
    "    presets=\"best_quality\",\n",
    "    auto_stack=True,\n",
    "    use_bag_holdout=True,\n",
    "    num_stack_levels=0,\n",
    "    time_limit=60*60*3.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train AutoGluon model\n",
    "tabular_predictor = TabularPredictor(label=\"pv_measurement\", eval_metric=\"mae\")\n",
    "\n",
    "predictor_B = tabular_predictor.fit(\n",
    "    train_data=train[train.location == \"B\"],\n",
    "    tuning_data=val[val.location == \"B\"],\n",
    "    presets=\"best_quality\",\n",
    "    auto_stack=True,\n",
    "    use_bag_holdout=True,\n",
    "    num_stack_levels=0,\n",
    "    time_limit=60*60*2.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train AutoGluon model\n",
    "tabular_predictor = TabularPredictor(label=\"pv_measurement\", eval_metric=\"mae\")\n",
    "\n",
    "predictor_C = tabular_predictor.fit(\n",
    "    train_data=train[train.location == \"C\"],\n",
    "    tuning_data=val[val.location == \"C\"],\n",
    "    presets=\"best_quality\",\n",
    "    auto_stack=True,\n",
    "    use_bag_holdout=True,\n",
    "    num_stack_levels=0,\n",
    "    time_limit=60*60*2.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test dataset\n",
    "test_data_nolab = test.drop(columns=[\"pv_measurement\"])\n",
    "\n",
    "y_pred_A = predictor_A.predict(\n",
    "    test_data_nolab[test_data_nolab.location == \"A\"]\n",
    ")\n",
    "y_pred_B = predictor_B.predict(\n",
    "    test_data_nolab[test_data_nolab.location == \"B\"]\n",
    ")\n",
    "y_pred_C = predictor_C.predict(\n",
    "    test_data_nolab[test_data_nolab.location == \"C\"]\n",
    ")\n",
    "\n",
    "# Add autogluon result to dataframe\n",
    "autogluon_result = pd.DataFrame(y_pred_A)\n",
    "autogluon_result[\"location\"] = \"A\"\n",
    "autogluon_result = pd.concat([autogluon_result, pd.DataFrame(y_pred_B)], axis=0)\n",
    "autogluon_result = autogluon_result.fillna(\"B\")\n",
    "autogluon_result = pd.concat([autogluon_result, pd.DataFrame(y_pred_C)], axis=0)\n",
    "autogluon_result = autogluon_result.fillna(\"C\")\n",
    "\n",
    "test_result[\"autogluon\"] = autogluon_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 6 – Location Specific FastAI Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_neural = data.copy()\n",
    "\n",
    "# Drop columns that are not needed\n",
    "drop_cols = [\n",
    "    \"cloud_base_agl:m\",\n",
    "    \"ceiling_height_agl:m\",\n",
    "    \"snow_density:kgm3\",\n",
    "    \"hour\",\n",
    "    \"month\",\n",
    "    \"date_calc\",\n",
    "    \"date_forecast\",\n",
    "    \"elevation:m\",\n",
    "    \"fresh_snow_12h:cm\",\n",
    "    \"year\",\n",
    "    \"day\",\n",
    "    \"global_rad\"\n",
    "]\n",
    "\n",
    "# Target transformation\n",
    "data_neural[\"pv_measurement\"] = data_neural[\"pv_measurement\"]\n",
    "\n",
    "data_neural[\"global_rad\"] = data_neural[\"direct_rad:W\"] + data_neural[\"diffuse_rad:W\"]\n",
    "\n",
    "\n",
    "data_neural[\"year\"] = data_neural[\"date_forecast\"].dt.year\n",
    "data_neural[\"day\"] = data_neural[\"date_forecast\"].dt.dayofyear\n",
    "\n",
    "data_neural[\"global_rad_-1\"] = (\n",
    "    data_neural.groupby([\"location\", \"year\", \"day\"])[\"global_rad\"].shift(-1).fillna(0)\n",
    ")\n",
    "\n",
    "data_neural[\"precip_5min:mm-2\"] = (\n",
    "    data_neural.groupby([\"location\", \"year\", \"day\"])[\"precip_5min:mm\"]\n",
    "    .shift(-2)\n",
    "    .fillna(0)\n",
    ")\n",
    "data_neural[\"rain_water:kgm2-2\"] = (\n",
    "    data_neural.groupby([\"location\", \"year\", \"day\"])[\"rain_water:kgm2\"]\n",
    "    .shift(-2)\n",
    "    .fillna(0)\n",
    ")\n",
    "data_neural[\"precip_5min:mm-2\"] = (\n",
    "    data_neural.groupby([\"location\", \"year\", \"day\"])[\"precip_5min:mm\"]\n",
    "    .shift(-2)\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "# Agregations\n",
    "\n",
    "data_neural[\"total_clouds_rolling_3\"] = (\n",
    "    data_neural.groupby([\"location\", \"year\", \"day\"])[\"total_cloud_cover:p\"]\n",
    "    .rolling(3)\n",
    "    .mean()\n",
    "    .fillna(0)\n",
    "    .reset_index()[\"total_cloud_cover:p\"]\n",
    ")\n",
    "data_neural[\"precip_5min_rolling_3\"] = (\n",
    "    data_neural.groupby([\"location\", \"year\", \"day\"])[\"precip_5min:mm\"]\n",
    "    .rolling(3)\n",
    "    .mean()\n",
    "    .fillna(0)\n",
    "    .reset_index()[\"precip_5min:mm\"]\n",
    ")\n",
    "\n",
    "# avg rainfall per month calculated from estimated/observed\n",
    "sum_rainfall = (\n",
    "    data_neural.loc[data_neural[\"data_type\"].isin([\"observed\", \"estimated\"])]\n",
    "    .groupby(\"month\")[\"precip_5min:mm\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"precip_5min:mm\": \"sum_rainfall\"})\n",
    ")\n",
    "\n",
    "data_neural = data_neural.merge(sum_rainfall, on=\"month\", how=\"left\")\n",
    "\n",
    "# avg tmp per month calculated from estimated/observed\n",
    "sum_wind = (\n",
    "    data_neural.loc[data_neural[\"data_type\"].isin([\"observed\", \"estimated\"])]\n",
    "    .groupby(\"month\")[\"wind_speed_10m:ms\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"wind_speed_10m:ms\": \"sum_wind\"})\n",
    ")\n",
    "\n",
    "\n",
    "data_neural = data_neural.merge(sum_wind, on=\"month\", how=\"left\")\n",
    "\n",
    "median_wind_hourly = (\n",
    "    data_neural.loc[data_neural[\"data_type\"].isin([\"observed\", \"estimated\"])]\n",
    "    .groupby([\"month\", \"hour\"])[\"wind_speed_10m:ms\"]\n",
    "    .median()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"wind_speed_10m:ms\": \"median_wind_hourly\"})\n",
    ")\n",
    "\n",
    "data_neural = data_neural.merge(median_wind_hourly, on=[\"month\", \"hour\"], how=\"left\")\n",
    "\n",
    "data_neural[\"estimated\"] = np.where(\n",
    "    data_neural.data_type.isin([\"estimated\", \"test\"]), 1, 0\n",
    ")\n",
    "\n",
    "data_neural = data_neural.drop(drop_cols, axis=1)\n",
    "\n",
    "ignore_cols = [\n",
    "    \"location\",\n",
    "    \"dew_or_rime:idx\",\n",
    "    \"is_day:idx\",\n",
    "    \"is_in_shadow:idx\",\n",
    "    \"pv_measurement\",\n",
    "    \"data_type\",\n",
    "    \"precip_type_5min:idx\",\n",
    "]\n",
    "\n",
    "cols = [col for col in data_neural.columns if col not in ignore_cols]\n",
    "\n",
    "train_df = (\n",
    "    data_neural[data_neural[\"data_type\"].isin([\"observed\", \"estimated\"])]\n",
    "    .copy()\n",
    "    .drop(columns=[\"data_type\"])\n",
    ")\n",
    "test_df = (\n",
    "    data_neural[data_neural[\"data_type\"] == \"test\"].copy().drop(columns=[\"data_type\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a DataFrame named 'df' containing your data\n",
    "dep_var = \"pv_measurement\"\n",
    "cat_names = [\n",
    "    \"estimated\",\n",
    "    \"dew_or_rime:idx\",\n",
    "    \"precip_type_5min:idx\",\n",
    "]  # List of categorical variable names if any\n",
    "cont_names = [\n",
    "    col for col in train_df.columns if col not in cat_names + [dep_var, \"location\"]\n",
    "]\n",
    "\n",
    "procs = [\n",
    "    fastai.tabular.all.Categorify,\n",
    "    fastai.tabular.all.FillMissing,\n",
    "    fastai.tabular.all.Normalize,\n",
    "]  # Data preprocessing steps\n",
    "\n",
    "all_preds = pd.DataFrame(columns=[\"prediction\"])\n",
    "\n",
    "importances = {\"A\": None, \"B\": None, \"C\": None}\n",
    "\n",
    "for location in (\"A\", \"B\", \"C\"):\n",
    "    fastai.tabular.all.set_seed(42)\n",
    "\n",
    "    splits = RandomSplitter(seed=42)(\n",
    "        train_df[train_df.location == location]\n",
    "        .copy()\n",
    "        .drop(\n",
    "            columns=[\"location\"],\n",
    "            errors=\"ignore\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    to = fastai.tabular.all.TabularPandas(\n",
    "        train_df.copy()[train_df.location == location].drop(\n",
    "            columns=[\"location\"],\n",
    "            errors=\"ignore\",\n",
    "        ),\n",
    "        procs=procs,\n",
    "        cat_names=cat_names,\n",
    "        cont_names=[\n",
    "            col\n",
    "            for col in train_df.columns\n",
    "            if col not in cat_names + [dep_var, \"location\"]\n",
    "        ],\n",
    "        y_names=dep_var,\n",
    "        splits=splits,\n",
    "        y_block=fastai.tabular.all.RegressionBlock(),\n",
    "    )\n",
    "\n",
    "    dls = to.dataloaders(bs=128, seed=42)\n",
    "\n",
    "    config = tabular_config(ps=0.2)\n",
    "    # layers=[512, 256, 128, 62],\n",
    "    learn = fastai.tabular.all.tabular_learner(\n",
    "        dls,\n",
    "        # config=config,\n",
    "        metrics=[fastai.losses.L1LossFlat()],\n",
    "        loss_func=fastai.losses.L1LossFlat(),\n",
    "        # layers=[512, 256, 128, 62],\n",
    "        y_range=(0, train_df[train_df.location == location].pv_measurement.max()),\n",
    "    )\n",
    "\n",
    "    lr = learn.lr_find()\n",
    "\n",
    "    learn.fit_one_cycle(20, lr)\n",
    "\n",
    "    tst_dl = learn.dls.test_dl(\n",
    "        test_df[test_df.location == location]\n",
    "        .copy()\n",
    "        .drop(\n",
    "            columns=[\"location\"],\n",
    "            errors=\"ignore\",\n",
    "        )\n",
    "    )\n",
    "    preds, _ = learn.get_preds(dl=tst_dl)\n",
    "\n",
    "    all_preds = pd.concat(\n",
    "        [all_preds, pd.DataFrame(preds.numpy(), columns=[\"prediction\"])]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result[\"neural_net\"] = all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 – Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint = (\n",
    "    data[data[\"data_type\"].isin([\"observed\"]) & data[\"month\"].isin([5, 6, 7])]\n",
    "    .groupby([\"month\", \"hour\", \"location\"])[\"pv_measurement\"]\n",
    "    .max()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_data = (\n",
    "    data[data.data_type.isin([\"test\"])]\n",
    "    .sort_values([\"location\", \"date_forecast\"])\n",
    "    .copy()\n",
    ").reset_index(drop=True)\n",
    "\n",
    "kaggle_data[\"pv_measurement\"] = ensamble\n",
    "kaggle_data[\"pv_measurement_post\"] = pd.merge(\n",
    "    kaggle_data, constraint, on=[\"hour\", \"month\", \"location\"], how=\"left\"\n",
    ")[\"pv_measurement_y\"].values\n",
    "kaggle_data[\"pv_measurement_post\"] = np.where(\n",
    "    kaggle_data[\"pv_measurement_post\"] < kaggle_data[\"pv_measurement\"],\n",
    "    kaggle_data[\"pv_measurement_post\"],\n",
    "    kaggle_data[\"pv_measurement\"],\n",
    ")\n",
    "kaggle_data[\"pv_measurement_post\"] = np.where(\n",
    "    kaggle_data[\"pv_measurement_post\"] < 4, 0, kaggle_data[\"pv_measurement_post\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(kaggle_data[\"pv_measurement\"] - kaggle_data[\"pv_measurement_post\"]) / 2160"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 – Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame(\n",
    "    kaggle_data[\"pv_measurement_post\"].values, columns=[\"prediction\"]\n",
    ")\n",
    "output_df = output_df.reset_index(names=\"id\")\n",
    "output_df.to_csv(\n",
    "    \"../data/results/\" + str(datetime.datetime.now()) + \"-ensamble.csv\", index=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
